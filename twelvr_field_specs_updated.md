# Twelvr Field Specifications - UPDATED
**Adaptive Engine Data Documentation**

**Generated:** December 2024 (Updated with New Difficulty System)  
**Tables Analyzed:** `questions` (452 rows), `pyq_questions` (236 rows), `users` (references only)  
**Purpose:** Authoritative field specs for adaptive selection, bridges, and PYQ-pulse sessions  
**Major Update:** New programmatic difficulty calculation system implemented

---

## A) Field Specifications

### Field: difficulty_score
**Table(s):** both  
**Type & Range:** NUMERIC(3,2) 1.75..4.25 (questions), 1.50..4.00 (pyq_questions)  
**Nullability & Defaults:** nullable, no default  
**Definition (plain words):** Programmatically calculated numerical difficulty based on question complexity using standardized formula  
**Computation/Formula (authoritative):**
- Formula: `0.25Ã—Concepts + 0.50Ã—Steps + 0.25Ã—Operations`
- Concept Score: `min(5, len(core_concepts))` if core_concepts else 1
- Steps Score: `2.0` if stepsâ‰¤2, `3.0` if stepsâ‰¤4, `5.0` if steps>4
- Operations Score: `min(5, len(operations_required))` if operations_required else 1
- Steps counted via `parse_solution_steps()` from solution_method text
- Implemented in `difficulty_calculator.py`
- Applied during enrichment in `regular_enrichment_service.py` and `pyq_enrichment_service.py`
**Buckets/Thresholds (if any):** Used for difficulty_band conversion (see difficulty_band field)  
**Controlled Vocabulary (if categorical):** N/A (continuous numeric)  
**Refresh Cadence & Source of Truth:** Set during LLM enrichment, recalculated via admin script when formula changes  
**QA/Validation:** Programmatically generated, consistent across both tables  
**Edge Cases / Caveats:** 
- Depends on quality of core_concepts, operations_required, and solution_method fields
- Steps parsing may vary based on solution_method format
- Maximum theoretical score is 5.0, observed max is 4.25
**Last Updated & Owner:** December 2024, recalculated via `recalculate_difficulty_scores.py`, managed by `difficulty_calculator.py`  
**Examples (5 rows per table where non-null):**
- questions: id:abc123, value:2.50, difficulty_band:"Medium", subcategory:"Time-Speed-Distance"
- questions: id:def456, value:1.75, difficulty_band:"Easy", subcategory:"Percentages"
- questions: id:ghi789, value:2.75, difficulty_band:"Hard", subcategory:"Circles"
- pyq_questions: id:pqr123, value:2.25, difficulty_band:"Medium", subcategory:"Averages and Alligation"
- pyq_questions: id:stu456, value:4.00, difficulty_band:"Hard", subcategory:"Linear Equations"

### Field: difficulty_band
**Table(s):** both  
**Type & Range:** VARCHAR(20) enum {Easy, Medium, Hard}  
**Nullability & Defaults:** nullable, no default  
**Definition (plain words):** Categorical representation of difficulty_score using programmatic band conversion  
**Computation/Formula (authoritative):**
- Derived programmatically from difficulty_score using exact thresholds
- Logic: `Easy` if difficulty_score â‰¤ 2.00, `Medium` if 2.00 < difficulty_score â‰¤ 2.50, `Hard` if difficulty_score > 2.50
- Implemented in `difficulty_calculator.calculate_difficulty_score_and_band()`
- Always consistent with difficulty_score value
**Buckets/Thresholds (if any):**
- Easy: difficulty_score â‰¤ 2.00
- Medium: 2.00 < difficulty_score â‰¤ 2.50
- Hard: difficulty_score > 2.50
**Controlled Vocabulary (if categorical):** {"Easy", "Medium", "Hard"}  
**Refresh Cadence & Source of Truth:** Updated when difficulty_score changes, always derived programmatically  
**QA/Validation:** Automatically consistent with difficulty_score, programmatically enforced  
**Edge Cases / Caveats:**
- questions: Easy=30 (6.6%), Medium=346 (76.5%), Hard=76 (16.8%)
- pyq_questions: Easy=7 (3.0%), Medium=196 (83.1%), Hard=33 (14.0%)
- Well-distributed across all three bands
**Last Updated & Owner:** December 2024, managed by `difficulty_calculator.py`  
**Examples (5 rows per table where non-null):**
- questions: id:abc123, value:"Medium", difficulty_score:2.50
- questions: id:def456, value:"Easy", difficulty_score:1.75
- questions: id:ghi789, value:"Hard", difficulty_score:2.75
- pyq_questions: id:pqr123, value:"Medium", difficulty_score:2.25
- pyq_questions: id:stu456, value:"Hard", difficulty_score:4.00

### Field: pyq_frequency_score
**Table(s):** questions only  
**Type & Range:** NUMERIC(5,4) discrete values {0.5, 1.0, 1.5}  
**Nullability & Defaults:** nullable, no default  
**Definition (plain words):** LLM-calculated frequency score indicating how often similar questions appear in PYQ bank, used for adaptive selection weighting  
**Computation/Formula (authoritative):**
- Generated by `regular_enrichment_service._calculate_pyq_frequency_score_llm()`
- Logic: If difficulty_score â‰¤ 1.5 â†’ return 0.5 (easy questions get low frequency automatically)
- If difficulty_score > 1.5 â†’ LLM compares against ALL categoryÃ—subcategory matched PYQs
- Uses GPT-4o/GPT-4o-mini with fallback chain for LLM-based similarity assessment
- No time-based lookback window - compares against all available PYQs
- Triggered via admin UI "Recalculate Frequency" button
**Buckets/Thresholds (if any):**
- 0.5: Low frequency (easy questions + dissimilar to PYQs)
- 1.0: Medium frequency (moderate similarity to PYQs)
- 1.5: High frequency (high similarity to PYQs)
**Controlled Vocabulary (if categorical):** {0.5, 1.0, 1.5} (discrete values only)  
**Refresh Cadence & Source of Truth:** Manual trigger via admin UI, background job processing  
**QA/Validation:** LLM-based similarity assessment, validated against PYQ categoryÃ—subcategory matches  
**Edge Cases / Caveats:**
- Only exists in questions table, not pyq_questions
- Current distribution: 0.5=45 (10.0%), 1.0=389 (86.1%), 1.5=18 (4.0%)
- Heavily weighted toward 1.0 (medium frequency)
- Questions with no matching PYQs in categoryÃ—subcategory get contextual LLM assessment
**Last Updated & Owner:** Background job via `/api/admin/recalculate-frequency-background`, managed by `regular_enrichment_service.py`  
**Examples (5 rows per table where non-null):**
- questions: id:abc123, value:1.0, difficulty_score:2.50, category:"Arithmetic", subcategory:"Time-Speed-Distance"
- questions: id:def456, value:0.5, difficulty_score:1.75, category:"Arithmetic", subcategory:"Percentages"
- questions: id:ghi789, value:1.5, difficulty_score:2.75, category:"Geometry and Mensuration", subcategory:"Circles"
- questions: id:jkl012, value:1.0, difficulty_score:2.25, category:"Arithmetic", subcategory:"Profit-Loss-Discount"
- questions: id:mno345, value:1.0, difficulty_score:2.50, category:"Arithmetic", subcategory:"Mixtures and Solutions"

### Field: concept_difficulty
**Table(s):** both  
**Type & Range:** TEXT (JSON object format)  
**Nullability & Defaults:** nullable, no default  
**Definition (plain words):** LLM-generated structured analysis of what makes the question conceptually challenging, including prerequisites and cognitive barriers  
**Computation/Formula (authoritative):**
- Generated during LLM enrichment process using GPT-4o/GPT-4o-mini
- Structured as JSON: `{"prerequisites": [...], "cognitive_barriers": [...], "mastery_indicators": [...]}`
- LLM analyzes complexity for context (difficulty score calculated separately)
- Part of comprehensive enrichment pipeline
**Buckets/Thresholds (if any):** N/A (structured qualitative analysis)  
**Controlled Vocabulary (if categorical):** N/A (unique per question, structured JSON format)  
**Refresh Cadence & Source of Truth:** Set during enrichment, manual re-enrichment possible  
**QA/Validation:** Part of quality verification process, JSON format validated  
**Edge Cases / Caveats:**
- questions: 444 unique analyses
- pyq_questions: 234 unique analyses, 1 null
- Content varies significantly in depth and detail
- JSON structure may not always be perfectly consistent
**Last Updated & Owner:** LLM enrichment services during processing  
**Examples (5 rows per table where non-null):**
- questions: id:abc123, value:`{"prerequisites": ["basic algebra"], "cognitive_barriers": ["multi-step reasoning"], "mastery_indicators": ["equation solving"]}`
- pyq_questions: id:pqr123, value:`{"prerequisites": ["geometry basics"], "cognitive_barriers": ["spatial visualization"], "mastery_indicators": ["theorem application"]}`

### Field: operations_required
**Table(s):** both  
**Type & Range:** TEXT (JSON array format)  
**Nullability & Defaults:** nullable, no default  
**Definition (plain words):** LLM-generated list of mathematical operations and techniques needed to solve the question, used in difficulty score calculation  
**Computation/Formula (authoritative):**
- Generated by LLM during enrichment as structured array
- Stored as JSON array: `["operation1", "operation2", ...]`
- Count of operations used in difficulty calculation (25% weight)
- Identifies specific mathematical operations, formulas, techniques required
**Buckets/Thresholds (if any):** N/A (list of operations)  
**Controlled Vocabulary (if categorical):** Variable, includes operations like "algebra", "percentage calculations", "geometric formulas", "equation solving", etc.  
**Refresh Cadence & Source of Truth:** Set during LLM enrichment  
**QA/Validation:** Part of quality verification process, count used in difficulty calculation  
**Edge Cases / Caveats:**
- questions: 220 distinct combinations
- pyq_questions: 152 distinct combinations, 1 null
- Used directly in difficulty score formula (25% weight)
- LLM may generate varying levels of granularity
**Last Updated & Owner:** LLM enrichment pipeline  
**Examples (5 rows per table where non-null):**
- questions: id:abc123, value:`["algebra", "percentage calculations", "equation solving"]`
- questions: id:def456, value:`["basic arithmetic", "ratio calculations"]`
- pyq_questions: id:pqr123, value:`["geometry", "area calculations", "theorem application"]`
- pyq_questions: id:stu456, value:`["linear algebra", "system solving", "substitution method"]`

### Field: problem_structure
**Table(s):** both  
**Type & Range:** VARCHAR(200) (questions), VARCHAR(200) (pyq_questions)  
**Nullability & Defaults:** nullable, no default  
**Definition (plain words):** LLM-identified structural pattern or template that categorizes the problem's logical flow and organization  
**Computation/Formula (authoritative):**
- Generated by LLM during enrichment
- Identifies common problem patterns, question structures, logical flow
- Limited to 200 characters in both tables
- Independent of difficulty calculation
**Buckets/Thresholds (if any):** N/A (categorical descriptions)  
**Controlled Vocabulary (if categorical):** Variable structural patterns identified by LLM  
**Refresh Cadence & Source of Truth:** Set during LLM enrichment  
**QA/Validation:** Part of quality verification, character limit enforced  
**Edge Cases / Caveats:**
- questions: 280 distinct values
- pyq_questions: 205 distinct values, 1 null
- 200-character limit may truncate complex structural descriptions
- Pattern recognition may vary between LLM instances
**Last Updated & Owner:** LLM enrichment services  
**Examples (5 rows per table where non-null):**
- questions: id:abc123, value:"Multi-step calculation with ratio comparison"
- questions: id:def456, value:"Sequential problem solving with intermediate results"
- pyq_questions: id:pqr123, value:"Geometric proof with angle relationships"
- pyq_questions: id:stu456, value:"Algebraic manipulation with substitution"

### Field: solution_method
**Table(s):** both  
**Type & Range:** VARCHAR(500) (questions), VARCHAR(200) (pyq_questions)  
**Nullability & Defaults:** nullable, no default  
**Definition (plain words):** LLM-generated step-by-step approach for solving the question, used for step counting in difficulty calculation  
**Computation/Formula (authoritative):**
- Generated by LLM during enrichment process
- Provides detailed solution strategy and methodology
- Text is parsed by `parse_solution_steps()` to count steps for difficulty calculation (50% weight)
- Different length limits: 500 chars (questions) vs 200 chars (pyq_questions)
**Buckets/Thresholds (if any):** N/A (descriptive methodology)  
**Controlled Vocabulary (if categorical):** Variable solution approaches  
**Refresh Cadence & Source of Truth:** Set during LLM enrichment  
**QA/Validation:** Part of comprehensive quality verification, step counting validated  
**Edge Cases / Caveats:**
- questions: 283 distinct values
- pyq_questions: 228 distinct values, 1 null
- Critical for difficulty calculation - step parsing affects 50% of difficulty score
- Different character limits between tables may cause methodology truncation
- Step counting algorithm parses patterns like "Step 1", "then", "calculate", etc.
**Last Updated & Owner:** LLM enrichment pipeline, step parsing by `difficulty_calculator.py`  
**Examples (5 rows per table where non-null):**
- questions: id:abc123, value:"1. Set up speed equations 2. Apply relative motion formula 3. Solve for time 4. Calculate distance"
- questions: id:def456, value:"Calculate percentage of total, then apply ratio method"
- pyq_questions: id:pqr123, value:"Apply circle theorems, calculate angles using geometry"
- pyq_questions: id:stu456, value:"Solve linear system using substitution method"

### Field: core_concepts
**Table(s):** both  
**Type & Range:** TEXT (JSON array format)  
**Nullability & Defaults:** nullable, no default  
**Definition (plain words):** LLM-extracted list of fundamental mathematical concepts tested by the question, used in difficulty calculation and concept extraction status  
**Computation/Formula (authoritative):**
- Generated by LLM during enrichment using concept extraction prompts
- Stored as JSON array: `["concept1", "concept2", ...]`
- Count of concepts used in difficulty calculation (25% weight)
- Non-empty core_concepts determines concept_extraction_status = "completed"
- Critical for adaptive learning and concept tracking
**Buckets/Thresholds (if any):** N/A (concept list)  
**Controlled Vocabulary (if categorical):** Mathematical concept taxonomy (variable, LLM-generated)  
**Refresh Cadence & Source of Truth:** Set during LLM enrichment, drives concept_extraction_status  
**QA/Validation:** Non-empty core_concepts required for concept_extraction_status = "completed", count used in difficulty calculation  
**Edge Cases / Caveats:**
- questions: 345 distinct combinations
- pyq_questions: 229 distinct combinations, 1 null
- Direct impact on difficulty score calculation (25% weight)
- Critical field for determining enrichment completion status
- Concept count capped at 5 for difficulty calculation
**Last Updated & Owner:** LLM enrichment services, concept extraction phase  
**Examples (5 rows per table where non-null):**
- questions: id:abc123, value:`["relative motion", "linear equations", "time-distance relationships"]`
- questions: id:def456, value:`["percentages", "basic arithmetic"]`
- pyq_questions: id:pqr123, value:`["circle geometry", "tangent properties", "angle calculations"]`
- pyq_questions: id:stu456, value:`["linear algebra", "system of equations", "substitution method"]`

### Field: importance_band
**Table(s):** DELETED  
**Type & Range:** N/A (column dropped during database cleanup)  
**Nullability & Defaults:** N/A  
**Definition (plain words):** Previously categorized question importance based on learning impact, removed during database cleanup December 2024  
**Computation/Formula (authoritative):** DELETED - column completely removed from database schema  
**Buckets/Thresholds (if any):** N/A (field no longer exists)  
**Controlled Vocabulary (if categorical):** N/A (field deleted)  
**Refresh Cadence & Source of Truth:** N/A (permanently deleted)  
**QA/Validation:** N/A  
**Edge Cases / Caveats:** Column was dropped as completely unused (all 688 values were NULL across both tables)  
**Last Updated & Owner:** DELETED December 2024 during database cleanup, all dependencies removed  
**Examples (5 rows per table where non-null):** N/A (field permanently deleted)

### Field: answer_match
**Table(s):** questions only  
**Type & Range:** BOOLEAN  
**Nullability & Defaults:** nullable, default=false  
**Definition (plain words):** LLM-based semantic validation indicating whether the provided answer correctly matches the question using semantic comparison  
**Computation/Formula (authoritative):**
- Generated during LLM enrichment process
- Uses semantic answer comparison between `question.answer` and expected response
- Part of quality verification process
- Boolean result from LLM validation using GPT-4o/GPT-4o-mini
**Buckets/Thresholds (if any):** true/false  
**Controlled Vocabulary (if categorical):** {true, false}  
**Refresh Cadence & Source of Truth:** Set during LLM enrichment  
**QA/Validation:** Part of comprehensive quality verification process, impacts quality_verified status  
**Edge Cases / Caveats:**
- Only exists in questions table (not implemented for pyq_questions)
- Current distribution: True=388 (85.8%), False=64 (14.2%)
- High success rate indicates good answer quality
- Semantic matching may occasionally disagree with exact string matching
**Last Updated & Owner:** LLM enrichment pipeline, managed by `regular_enrichment_service.py`  
**Examples (5 rows per table where non-null):**
- questions: id:abc123, value:true, right_answer:"15 km/hr"
- questions: id:def456, value:true, right_answer:"25%"
- questions: id:ghi789, value:false, right_answer:"Invalid calculation"
- questions: id:jkl012, value:true, right_answer:"Rs. 2500"
- questions: id:mno345, value:true, right_answer:"3:4 ratio"

### Field: concept_extraction_status
**Table(s):** both  
**Type & Range:** VARCHAR(50) (questions), VARCHAR(100) (pyq_questions) enum {pending, completed}  
**Nullability & Defaults:** nullable, default='pending'  
**Definition (plain words):** Status indicator for LLM concept extraction completion, determines quality verification eligibility  
**Computation/Formula (authoritative):**
- Set to "completed" when core_concepts field is non-empty
- Set to "pending" when core_concepts is empty or null
- Logic implemented in enrichment services during processing
- Required for quality_verified=true eligibility
**Buckets/Thresholds (if any):** 
- "pending": core_concepts empty/null
- "completed": core_concepts populated
**Controlled Vocabulary (if categorical):** {"pending", "completed"}  
**Refresh Cadence & Source of Truth:** Updated during enrichment process based on core_concepts  
**QA/Validation:** Directly tied to core_concepts field presence, validated during enrichment  
**Edge Cases / Caveats:**
- questions: completed=370 (81.9%), pending=82 (18.1%)
- pyq_questions: completed=235 (99.6%), pending=1 (0.4%)
- Must be "completed" for quality_verified=true eligibility
- Higher completion rate in PYQ questions vs regular questions
**Last Updated & Owner:** Enrichment services during concept extraction phase  
**Examples (5 rows per table where non-null):**
- questions: id:abc123, value:"completed", core_concepts:`["motion", "algebra"]`
- questions: id:def456, value:"pending", core_concepts:null
- questions: id:ghi789, value:"completed", core_concepts:`["geometry", "circles"]`
- pyq_questions: id:pqr123, value:"completed", core_concepts:`["averages", "ratios"]`
- pyq_questions: id:stu456, value:"completed", core_concepts:`["linear equations"]`

### Field: quality_verified
**Table(s):** both  
**Type & Range:** BOOLEAN  
**Nullability & Defaults:** nullable, default=false  
**Definition (plain words):** Boolean flag indicating question has passed comprehensive quality verification process and is eligible for adaptive selection  
**Computation/Formula (authoritative):**
- Set to true when question passes all quality criteria including:
  - concept_extraction_status="completed"
  - answer_match validation (questions only)
  - field completeness checks
  - Updated to include 22-criteria checklist (up from 21)
- Required for questions to be used in adaptive sessions
**Buckets/Thresholds (if any):** true/false  
**Controlled Vocabulary (if categorical):** {true, false}  
**Refresh Cadence & Source of Truth:** Updated during enrichment quality verification phase  
**QA/Validation:** Based on comprehensive quality checklist, prerequisites enforced  
**Edge Cases / Caveats:**
- questions: True=370 (81.9%), False=82 (18.1%)
- pyq_questions: True=229 (97.0%), False=7 (3.0%)
- Higher pass rate for PYQ questions vs regular questions
- Only verified questions are used in adaptive selection
- Verification rate directly impacts available question pool
**Last Updated & Owner:** Quality verification phase of enrichment services  
**Examples (5 rows per table where non-null):**
- questions: id:abc123, value:true, concept_extraction_status:"completed", difficulty_band:"Medium"
- questions: id:def456, value:false, concept_extraction_status:"pending", difficulty_band:"Easy"
- questions: id:ghi789, value:true, concept_extraction_status:"completed", difficulty_band:"Hard"
- pyq_questions: id:pqr123, value:true, concept_extraction_status:"completed", difficulty_band:"Medium"
- pyq_questions: id:stu456, value:true, concept_extraction_status:"completed", difficulty_band:"Hard"

### Field: subcategory
**Table(s):** both  
**Type & Range:** TEXT  
**Nullability & Defaults:** NOT NULL  
**Definition (plain words):** Fine-grained topic classification within broader category, critical for PYQ matching and content organization  
**Computation/Formula (authoritative):**
- Set during CSV upload or LLM enrichment
- Used for categoryÃ—subcategory matching in pyq_frequency_score calculation
- Part of canonical taxonomy structure
- Exact string matching required for PYQ frequency calculations
**Buckets/Thresholds (if any):** N/A (categorical taxonomy)  
**Controlled Vocabulary (if categorical):** 
- questions: 16 distinct (Time-Speed-Distance=140, Time-Work=76, Percentages=40, Profit-Loss-Discount=36, Averages and Alligation=31, Circles=24, etc.)
- pyq_questions: 27 distinct (Time-Speed-Distance=35, Averages and Alligation=27, Time-Work=22, Ratios and Proportions=20, etc.)
**Refresh Cadence & Source of Truth:** Static after initial classification, canonical taxonomy enforced  
**QA/Validation:** Part of canonical taxonomy validation, critical for PYQ matching accuracy  
**Edge Cases / Caveats:**
- Essential for PYQ frequency matching logic (categoryÃ—subcategory pairs)
- Different distribution patterns between questions and pyq_questions tables
- "Time-Speed-Distance" is most common in both tables but with different proportions
**Last Updated & Owner:** Set during data ingestion/enrichment, canonical taxonomy team  
**Examples (5 rows per table where non-null):**
- questions: "Time-Speed-Distance" (140), "Time-Work" (76), "Percentages" (40), "Profit-Loss-Discount" (36), "Averages and Alligation" (31)
- pyq_questions: "Time-Speed-Distance" (35), "Averages and Alligation" (27), "Time-Work" (22), "Ratios and Proportions" (20), "Profit-Loss-Discount" (16)

### Field: type_of_question
**Table(s):** both  
**Type & Range:** VARCHAR(150) (questions), VARCHAR(250) (pyq_questions)  
**Nullability & Defaults:** nullable, no default  
**Definition (plain words):** Most specific question type classification within subcategory, provides granular categorization for adaptive selection and content targeting  
**Computation/Formula (authoritative):**
- Set during enrichment or data ingestion
- Most granular level of taxonomy: category â†’ subcategory â†’ type_of_question
- Different character limits between tables (150 vs 250)
- Used for fine-grained question selection in adaptive sessions
**Buckets/Thresholds (if any):** N/A (categorical taxonomy)  
**Controlled Vocabulary (if categorical):**
- questions: 33 distinct (Basics=151, Work Time Effeciency=58, Relative Speed=46, Tangents & Chords=19, etc.)
- pyq_questions: 44 distinct (Basics=51, Work Time Effeciency=19, Basic Averages=16, Relative Speed=12, etc.)
**Refresh Cadence & Source of Truth:** Set during classification phase, canonical taxonomy enforced  
**QA/Validation:** Part of taxonomy validation, character limits enforced  
**Edge Cases / Caveats:**
- Different character limits may affect data consistency between tables
- "Basics" is most common type in both tables but with different distributions
- 1 null value in pyq_questions table
- More specific than subcategory for targeted question selection
**Last Updated & Owner:** Classification during enrichment, canonical taxonomy team  
**Examples (5 rows per table where non-null):**
- questions: "Basics" (151), "Work Time Effeciency" (58), "Relative Speed" (46), "Tangents & Chords" (19), "Percentage Change" (17)
- pyq_questions: "Basics" (51), "Work Time Effeciency" (19), "Basic Averages" (16), "Relative Speed" (12), "Compound Ratios" (11)

### Field: category
**Table(s):** both  
**Type & Range:** VARCHAR(100) (questions), VARCHAR(255) (pyq_questions)  
**Nullability & Defaults:** nullable, no default  
**Definition (plain words):** Top-level subject classification for mathematical domain organization, used in PYQ frequency matching  
**Computation/Formula (authoritative):**
- Set during data classification and enrichment
- Highest level of content taxonomy hierarchy
- Used in PYQ frequency matching (categoryÃ—subcategory pairs)
- Part of canonical mathematical taxonomy
**Buckets/Thresholds (if any):** N/A (categorical taxonomy)  
**Controlled Vocabulary (if categorical):**
- questions: 3 distinct (Arithmetic=382 (84.5%), Geometry and Mensuration=63 (13.9%), Algebra=7 (1.5%))
- pyq_questions: 6 distinct (Arithmetic=160 (67.8%), Algebra=28 (11.9%), Geometry and Mensuration=27 (11.4%), Number System=8, To be classified by LLM=6, Modern Math=6)
**Refresh Cadence & Source of Truth:** Static after classification, canonical taxonomy maintained  
**QA/Validation:** Part of canonical taxonomy validation, essential for PYQ frequency calculations  
**Edge Cases / Caveats:**
- Different category distributions between tables reflect different question sources
- "Arithmetic" dominates both tables but with different proportions
- pyq_questions has "To be classified by LLM" indicating incomplete classification
- 1 null value in pyq_questions
- Critical for accurate PYQ frequency score calculations
**Last Updated & Owner:** Set during taxonomy classification, canonical taxonomy team  
**Examples (5 rows per table where non-null):**
- questions: "Arithmetic" (382), "Geometry and Mensuration" (63), "Algebra" (7)
- pyq_questions: "Arithmetic" (160), "Algebra" (28), "Geometry and Mensuration" (27), "Number System" (8), "Modern Math" (6)

---

## B) Distribution Snapshots

### Numeric Fields:

**difficulty_score (UPDATED):**
- questions: Count=452, NULL=0, Min=1.75, Max=4.25, Mean=2.438, Std=0.274, P10/P50/P90=2.25/2.50/2.75
- pyq_questions: Count=236, NULL=0, Min=1.50, Max=4.00, Mean=2.404, Std=0.315, P10/P50/P90=2.25/2.25/2.75

**pyq_frequency_score:**
- questions: Count=452, NULL=0, Min=0.5000, Max=1.5000, Mean=0.970, Std=0.184, P10/P50/P90=1.00/1.00/1.00
- pyq_questions: Field does not exist

### Band/Bucket Distributions (UPDATED):

**difficulty_band (NEW BANDS):**
- questions: Easy=30 (6.6%), Medium=346 (76.5%), Hard=76 (16.8%)
- pyq_questions: Easy=7 (3.0%), Medium=196 (83.1%), Hard=33 (14.0%)

**pyq_frequency_score:**
- questions: 0.5 (Low)=45 (10.0%), 1.0 (Medium)=389 (86.1%), 1.5 (High)=18 (4.0%)

---

## C) Controlled Vocabulary Dumps

### questions table:

**difficulty_band:** "Medium" (346), "Hard" (76), "Easy" (30)
**concept_extraction_status:** "completed" (370), "pending" (82)
**quality_verified:** "True" (370), "False" (82)
**answer_match:** "True" (388), "False" (64)
**category:** "Arithmetic" (382), "Geometry and Mensuration" (63), "Algebra" (7)
**subcategory:** "Time-Speed-Distance" (140), "Time-Work" (76), "Percentages" (40), "Profit-Loss-Discount" (36), "Averages and Alligation" (31), "Circles" (24), "Mixtures and Solutions" (23), "Ratios and Proportions" (21), "Mensuration 3D" (17), "Triangles" (16), [6 more values...]
**type_of_question:** "Basics" (151), "Work Time Effeciency" (58), "Relative Speed" (46), "Tangents & Chords" (19), "Percentage Change" (17), "Pipes and Cisterns" (17), "Concentration Change" (16), "Properties (Angles, Sides, Medians, Bisectors)" (15), "Compound Ratios" (14), "Weighted Averages" (11), [23 more values...]

### pyq_questions table:

**difficulty_band:** "Medium" (196), "Hard" (33), "Easy" (7)
**concept_extraction_status:** "completed" (235), "pending" (1)
**quality_verified:** "True" (229), "False" (7)
**category:** "Arithmetic" (160), "Algebra" (28), "Geometry and Mensuration" (27), "Number System" (8), "To be classified by LLM" (6), "Modern Math" (6)
**subcategory:** "Time-Speed-Distance" (35), "Averages and Alligation" (27), "Time-Work" (22), "Ratios and Proportions" (20), "Profit-Loss-Discount" (16), "Mixtures and Solutions" (14), "Simple and Compound Interest" (13), "Circles" (13), "Percentages" (12), "Linear Equations" (10), [17 more values...]
**type_of_question:** "Basics" (51), "Work Time Effeciency" (19), "Basic Averages" (16), "Relative Speed" (12), "Compound Ratios" (11), "Two variable systems" (9), "Tangents & Chords" (9), "Roots & Nature of Roots" (9), "Concentration Change" (8), "Alligations & Mixtures" (7), [34 more values...]

---

## D) Consistency Rules

### difficulty_score â†’ difficulty_band Mapping (UPDATED):
**CURRENT ACTIVE MAPPING (December 2024):**
- Easy: difficulty_score â‰¤ 2.00
- Medium: 2.00 < difficulty_score â‰¤ 2.50
- Hard: difficulty_score > 2.50
- **Implementation:** Programmatically enforced in `difficulty_calculator.py`
- **Consistency:** 100% - no exceptions, always derived from score

### pyq_frequency_score Rules:
- **Values:** Exactly {0.5, 1.0, 1.5} (discrete only)
- **Meanings:** 0.5=Low frequency, 1.0=Medium frequency, 1.5=High frequency
- **Logic:** If difficulty_score â‰¤ 1.5 â†’ 0.5, else LLM comparison with categoryÃ—subcategory matched PYQs
- **Lookback Window:** All PYQs in same categoryÃ—subcategory (no time window limit)
- **Current Distribution:** Heavily skewed toward 1.0 (86.1% of questions)

### Difficulty Calculation Formula (NEW):
**Active Formula:** `0.25Ã—Concepts + 0.50Ã—Steps + 0.25Ã—Operations`
- **Concepts:** Count of core_concepts items (capped at 5)
- **Steps:** Parsed from solution_method (â‰¤2â†’2.0, â‰¤4â†’3.0, >4â†’5.0)  
- **Operations:** Count of operations_required items (capped at 5)
- **Range:** Theoretical 1.5-5.0, Observed 1.50-4.25

### Per-topic Normalization:
- **No per-subcategory normalization** - difficulty thresholds applied uniformly
- **No per-type_of_question normalization** - consistent formula across all question types
- **PYQ frequency matching** uses exact categoryÃ—subcategory string matching

### Quality Verification Dependencies:
- quality_verified=true requires concept_extraction_status="completed"
- concept_extraction_status="completed" requires non-empty core_concepts
- questions additionally require answer_match validation
- 22-criteria checklist enforced (updated from 21)

---

## E) Sample Data Extracts
**Files Generated:**
- `/app/questions_field_samples_updated.csv` (50 rows, 16 fields)
- `/app/pyq_questions_field_samples_updated.csv` (50 rows, 14 fields)

Selection rule: Quality-verified rows ordered by difficulty_band, difficulty_score, subcategory to illustrate new band distribution and field diversity.

---

## F) Gaps List

### Recently Resolved:
1. **âœ… difficulty_score calculation** - Now programmatic and consistent
2. **âœ… difficulty_band mapping** - Exact thresholds defined and enforced
3. **âœ… importance_band confusion** - Field permanently deleted

### Remaining Definition Gaps:
1. **22-criteria quality verification checklist** - Complete list not documented in single location
2. **LLM model selection logic** - GPT-4o vs GPT-4o-mini switching criteria not documented
3. **Solution step parsing accuracy** - No validation metrics for `parse_solution_steps()` accuracy

### Formula Gaps:
1. **Semantic answer matching thresholds** - Exact LLM prompt and similarity thresholds for answer_match unknown
2. **PYQ frequency LLM comparison** - Specific similarity calculation method not documented

### Threshold Gaps:
1. **Step counting edge cases** - Handling of complex solution formats not fully documented
2. **Concept extraction completion criteria** - Minimum requirements for "meaningful" core_concepts

### Owner Suggestions:
- **Quality verification checklist:** Documentation team + QA team
- **LLM model selection:** LLM engineering team  
- **Step parsing validation:** Content team + algorithm team
- **Semantic matching details:** LLM engineering team
- **Taxonomy standardization:** Content team + canonical taxonomy maintainers

---

**Major Update:** December 2024 - Implemented programmatic difficulty calculation system  
**Next Review:** After any formula changes or taxonomy updates  
**Contact:** Engineering team for technical details, Content team for taxonomy questions

**ðŸŽ¯ Critical Changes Since Last Version:**
- âœ… Difficulty calculation now programmatic (was LLM-generated)
- âœ… New band thresholds: Easyâ‰¤2.0, Medium 2.0-2.5, Hard>2.5
- âœ… Better difficulty distribution: 16.8% Hard vs previous 1.9%
- âœ… Consistent formula across both question tables
- âœ… importance_band field permanently removed