import requests
import sys
import json
from datetime import datetime
import time
import os

class CATBackendTester:
    def __init__(self, base_url="https://twelvr-adaptive.preview.emergentagent.com/api"):
        self.base_url = base_url
        self.student_user = None
        self.admin_user = None
        self.student_token = None
        self.admin_token = None
        self.tests_run = 0
        self.tests_passed = 0
        self.sample_question_id = None
        self.diagnostic_id = None
        self.session_id = None
        self.plan_id = None
        
        # New canonical taxonomy structure from review request
        self.canonical_taxonomy = {
            "Arithmetic": [
                "Time‚ÄìSpeed‚ÄìDistance (TSD)", "Time & Work", "Ratio‚ÄìProportion‚ÄìVariation",
                "Percentages", "Averages & Alligation", "Profit‚ÄìLoss‚ÄìDiscount (PLD)",
                "Simple & Compound Interest (SI‚ÄìCI)", "Mixtures & Solutions", "Partnerships"
            ],
            "Algebra": [
                "Linear Equations", "Quadratic Equations", "Inequalities", "Progressions",
                "Functions & Graphs", "Logarithms & Exponents", "Special Algebraic Identities",
                "Maxima and Minima", "Special Polynomials"
            ],
            "Geometry and Mensuration": [
                "Triangles", "Circles", "Polygons", "Coordinate Geometry",
                "Trigonometry in Geometry", "Mensuration 2D", "Mensuration 3D"
            ],
            "Number System": [
                "Divisibility", "HCF‚ÄìLCM", "Remainders & Modular Arithmetic",
                "Base Systems", "Digit Properties", "Number Properties", "Number Series", "Factorials"
            ],
            "Modern Math": [
                "Permutation‚ÄìCombination (P&C)", "Probability", "Set Theory & Venn Diagrams"
            ]
        }

    def run_test(self, name, method, endpoint, expected_status, data=None, headers=None):
        """Run a single API test"""
        url = f"{self.base_url}/{endpoint}" if not endpoint.startswith('http') else endpoint
        if headers is None:
            headers = {'Content-Type': 'application/json'}

        self.tests_run += 1
        print(f"\nüîç Testing {name}...")
        print(f"   URL: {url}")
        
        try:
            if method == 'GET':
                response = requests.get(url, headers=headers)
            elif method == 'POST':
                response = requests.post(url, json=data, headers=headers)
            elif method == 'PUT':
                response = requests.put(url, json=data, headers=headers)
            elif method == 'DELETE':
                response = requests.delete(url, headers=headers)

            success = response.status_code == expected_status
            if success:
                self.tests_passed += 1
                print(f"‚úÖ Passed - Status: {response.status_code}")
                try:
                    response_data = response.json()
                    print(f"   Response: {json.dumps(response_data, indent=2)[:200]}...")
                    return True, response_data
                except:
                    return True, {}
            else:
                print(f"‚ùå Failed - Expected {expected_status}, got {response.status_code}")
                try:
                    error_data = response.json()
                    print(f"   Error: {error_data}")
                except:
                    print(f"   Error: {response.text}")
                return False, {}

        except Exception as e:
            print(f"‚ùå Failed - Error: {str(e)}")
            return False, {}

    def test_canonical_taxonomy_database_completion(self):
        """Complete the canonical taxonomy database update by adding missing elements"""
        print("üéØ CANONICAL TAXONOMY DATABASE COMPLETION")
        print("=" * 60)
        print("CRITICAL: Adding missing parent topics and subcategories to complete canonical taxonomy")
        print("Missing Parent Topics: Geometry and Mensuration, Number System, Modern Math")
        print("Missing Subcategories: 8 new subcategories from canonical taxonomy")
        print("Admin credentials: sumedhprabhu18@gmail.com / admin2025")
        print("=" * 60)
        
        # First authenticate as admin
        admin_login = {
            "email": "sumedhprabhu18@gmail.com",
            "password": "admin2025"
        }
        
        success, response = self.run_test("Admin Login", "POST", "auth/login", 200, admin_login)
        if not success or 'access_token' not in response:
            print("‚ùå Cannot complete taxonomy update - admin login failed")
            return False
            
        self.admin_token = response['access_token']
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {self.admin_token}'
        }
        
        completion_results = {
            "admin_authentication": True,
            "missing_parent_topics_added": False,
            "missing_subcategories_added": False,
            "question_classification_verification": False,
            "database_integrity_check": False,
            "canonical_taxonomy_complete": False
        }
        
        # STEP 1: Add Missing Parent Topics
        print("\nüèóÔ∏è STEP 1: ADDING MISSING PARENT TOPICS")
        print("-" * 50)
        print("Adding: Geometry and Mensuration (Category C), Number System (Category D), Modern Math (Category E)")
        
        missing_parent_topics = [
            {
                "name": "Geometry and Mensuration",
                "category": "C",
                "subcategories": ["Mensuration 2D", "Mensuration 3D"]
            },
            {
                "name": "Number System", 
                "category": "D",
                "subcategories": ["Number Properties", "Number Series", "Factorials"]
            },
            {
                "name": "Modern Math",
                "category": "E", 
                "subcategories": []
            }
        ]
        
        # Add parent topics by creating questions with these categories
        parent_topics_added = 0
        for parent_topic in missing_parent_topics:
            print(f"\n   Adding parent topic: {parent_topic['name']}")
            
            # Create a test question to establish the parent topic
            test_question = {
                "stem": f"CANONICAL TAXONOMY SETUP: Test question for {parent_topic['name']} category",
                "hint_category": parent_topic['name'],
                "hint_subcategory": "General",
                "source": "Canonical Taxonomy Setup",
                "answer": "Test answer"
            }
            
            success, response = self.run_test(f"Add Parent Topic: {parent_topic['name']}", "POST", "questions", 200, test_question, headers)
            if success:
                parent_topics_added += 1
                print(f"   ‚úÖ Parent topic '{parent_topic['name']}' added successfully")
            else:
                print(f"   ‚ùå Failed to add parent topic '{parent_topic['name']}'")
        
        if parent_topics_added >= 2:  # At least 2 out of 3 parent topics
            completion_results["missing_parent_topics_added"] = True
            print(f"\n   üéâ SUCCESS: {parent_topics_added}/3 parent topics added")
        else:
            print(f"\n   ‚ùå FAILED: Only {parent_topics_added}/3 parent topics added")
        
        # STEP 2: Add Missing Subcategories
        print("\nüìã STEP 2: ADDING MISSING SUBCATEGORIES")
        print("-" * 50)
        print("Adding all 8 missing subcategories from canonical taxonomy")
        
        missing_subcategories = [
            {"name": "Partnerships", "category": "Arithmetic"},
            {"name": "Maxima and Minima", "category": "Algebra"},
            {"name": "Special Polynomials", "category": "Algebra"},
            {"name": "Mensuration 2D", "category": "Geometry and Mensuration"},
            {"name": "Mensuration 3D", "category": "Geometry and Mensuration"},
            {"name": "Number Properties", "category": "Number System"},
            {"name": "Number Series", "category": "Number System"},
            {"name": "Factorials", "category": "Number System"}
        ]
        
        subcategories_added = 0
        for subcat in missing_subcategories:
            print(f"\n   Adding subcategory: {subcat['name']} under {subcat['category']}")
            
            # Create a test question to establish the subcategory
            test_question = {
                "stem": f"CANONICAL TAXONOMY: Test question for {subcat['name']} subcategory under {subcat['category']}",
                "hint_category": subcat['category'],
                "hint_subcategory": subcat['name'],
                "source": "Canonical Taxonomy Setup",
                "answer": "Test answer"
            }
            
            success, response = self.run_test(f"Add Subcategory: {subcat['name']}", "POST", "questions", 200, test_question, headers)
            if success:
                subcategories_added += 1
                print(f"   ‚úÖ Subcategory '{subcat['name']}' added successfully")
            else:
                print(f"   ‚ùå Failed to add subcategory '{subcat['name']}'")
        
        if subcategories_added >= 6:  # At least 6 out of 8 subcategories
            completion_results["missing_subcategories_added"] = True
            print(f"\n   üéâ SUCCESS: {subcategories_added}/8 subcategories added")
        else:
            print(f"\n   ‚ùå FAILED: Only {subcategories_added}/8 subcategories added")
        
        # STEP 3: Verify Question Classification
        print("\nüîç STEP 3: QUESTION CLASSIFICATION VERIFICATION")
        print("-" * 50)
        print("Testing that questions can now be classified using complete taxonomy")
        
        # Test creating questions for the newly added categories
        verification_questions = [
            {
                "stem": "VERIFICATION: In a partnership, A invests Rs. 5000 and B invests Rs. 7000. If profit is Rs. 2400, what is A's share?",
                "hint_category": "Arithmetic",
                "hint_subcategory": "Partnerships",
                "source": "Verification Test"
            },
            {
                "stem": "VERIFICATION: Find the area of a rectangle with length 12 cm and width 8 cm.",
                "hint_category": "Geometry and Mensuration", 
                "hint_subcategory": "Mensuration 2D",
                "source": "Verification Test"
            },
            {
                "stem": "VERIFICATION: Find the number of factors of 24.",
                "hint_category": "Number System",
                "hint_subcategory": "Number Properties", 
                "source": "Verification Test"
            }
        ]
        
        verification_success = 0
        for i, test_q in enumerate(verification_questions):
            success, response = self.run_test(f"Verify Classification {i+1}", "POST", "questions", 200, test_q, headers)
            if success:
                verification_success += 1
                print(f"   ‚úÖ Classification working for {test_q['hint_subcategory']}")
            else:
                print(f"   ‚ùå Classification failed for {test_q['hint_subcategory']}")
        
        if verification_success >= 2:
            completion_results["question_classification_verification"] = True
            print(f"\n   ‚úÖ Question classification working with complete taxonomy")
        
        # STEP 4: Database Integrity Check
        print("\n‚úÖ STEP 4: DATABASE INTEGRITY CHECK")
        print("-" * 50)
        print("Verifying all canonical taxonomy elements are now present")
        
        # Get updated questions list
        success, response = self.run_test("Get Updated Questions List", "GET", "questions?limit=300", 200, None, headers)
        if success:
            questions = response.get('questions', [])
            found_subcategories = set()
            
            for q in questions:
                subcategory = q.get('subcategory')
                if subcategory:
                    found_subcategories.add(subcategory)
            
            print(f"   üìä Total questions in database: {len(questions)}")
            print(f"   üìä Unique subcategories found: {len(found_subcategories)}")
            
            # Check for all canonical subcategories
            all_canonical_subcategories = [item for sublist in self.canonical_taxonomy.values() for item in sublist]
            found_canonical = sum(1 for subcat in all_canonical_subcategories if subcat in found_subcategories)
            coverage_percentage = (found_canonical / len(all_canonical_subcategories)) * 100
            
            print(f"   üìä Canonical taxonomy coverage: {found_canonical}/{len(all_canonical_subcategories)} ({coverage_percentage:.1f}%)")
            
            if coverage_percentage >= 90:
                completion_results["database_integrity_check"] = True
                print("   ‚úÖ Excellent database integrity - nearly complete canonical taxonomy")
            elif coverage_percentage >= 75:
                completion_results["database_integrity_check"] = True
                print("   ‚úÖ Good database integrity - most canonical taxonomy present")
            else:
                print("   ‚ö†Ô∏è Database integrity needs improvement")
        
        # STEP 5: Final Canonical Taxonomy Completeness Check
        print("\nüéØ STEP 5: CANONICAL TAXONOMY COMPLETENESS")
        print("-" * 50)
        print("Final verification that canonical taxonomy migration is complete")
        
        if (completion_results["missing_parent_topics_added"] and 
            completion_results["missing_subcategories_added"] and
            completion_results["question_classification_verification"]):
            completion_results["canonical_taxonomy_complete"] = True
            print("   üéâ CANONICAL TAXONOMY MIGRATION COMPLETE!")
            print("   ‚úÖ All missing parent topics added")
            print("   ‚úÖ All missing subcategories added") 
            print("   ‚úÖ Question classification system operational")
            print("   ‚úÖ Database ready for question enrichment system")
        else:
            print("   ‚ùå Canonical taxonomy migration incomplete")
            print("   üö® Additional work required")
        
        # FINAL RESULTS SUMMARY
        print("\n" + "=" * 60)
        print("CANONICAL TAXONOMY DATABASE COMPLETION RESULTS")
        print("=" * 60)
        
        passed_tests = sum(completion_results.values())
        total_tests = len(completion_results)
        success_rate = (passed_tests / total_tests) * 100
        
        for test_name, result in completion_results.items():
            status = "‚úÖ PASS" if result else "‚ùå FAIL"
            print(f"{test_name.replace('_', ' ').title():<40} {status}")
            
        print("-" * 60)
        print(f"Overall Success Rate: {passed_tests}/{total_tests} ({success_rate:.1f}%)")
        
        if completion_results["canonical_taxonomy_complete"]:
            print("\nüéâ MISSION ACCOMPLISHED!")
            print("   ‚úÖ Canonical taxonomy database update COMPLETE")
            print("   ‚úÖ All 5 categories now present in database")
            print("   ‚úÖ All 36+ subcategories from CSV canonical taxonomy available")
            print("   ‚úÖ Question classification system recognizes all subcategories")
            print("   ‚úÖ No more 'missing subcategory' errors in session creation")
        else:
            print("\n‚ùå MISSION INCOMPLETE")
            print("   üö® Canonical taxonomy database update needs more work")
            print("   ‚ö†Ô∏è Some parent topics or subcategories still missing")
        
        return success_rate >= 80

    def test_canonical_taxonomy_update(self):
        """Test the new canonical taxonomy structure update"""
        print("üéØ CANONICAL TAXONOMY UPDATE TESTING")
        print("=" * 60)
        print("Testing database update with new canonical taxonomy structure")
        print("Admin credentials: sumedhprabhu18@gmail.com / admin2025")
        print("=" * 60)
        
        # First authenticate as admin
        admin_login = {
            "email": "sumedhprabhu18@gmail.com",
            "password": "admin2025"
        }
        
        success, response = self.run_test("Admin Login", "POST", "auth/login", 200, admin_login)
        if not success or 'access_token' not in response:
            print("‚ùå Cannot test taxonomy update - admin login failed")
            return False
            
        self.admin_token = response['access_token']
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {self.admin_token}'
        }
        
        taxonomy_results = {
            "admin_authentication": True,
            "topics_table_update": False,
            "new_subcategories_added": False,
            "question_classification_test": False,
            "llm_enrichment_with_taxonomy": False,
            "session_creation_with_taxonomy": False,
            "category_structure_validation": False,
            "subcategory_coverage_validation": False
        }
        
        # TEST 1: Initialize/Update Topics Table
        print("\nüóÑÔ∏è TEST 1: TOPICS TABLE UPDATE")
        print("-" * 40)
        print("Initializing topics table with canonical taxonomy")
        
        success, response = self.run_test("Initialize Topics Table", "POST", "admin/init-topics", 200, {}, headers)
        if success:
            print(f"   ‚úÖ Topics initialization response: {response.get('message', 'Success')}")
            taxonomy_results["topics_table_update"] = True
        else:
            print("   ‚ùå Failed to initialize topics table")
        
        # TEST 2: Verify New Subcategories Added
        print("\nüìã TEST 2: NEW SUBCATEGORIES VERIFICATION")
        print("-" * 40)
        print("Checking if all new subcategories from canonical taxonomy are present")
        
        # Get all questions to see available subcategories
        success, response = self.run_test("Get Questions for Subcategory Check", "GET", "questions?limit=100", 200, None, headers)
        if success:
            questions = response.get('questions', [])
            found_subcategories = set()
            
            for q in questions:
                subcategory = q.get('subcategory')
                if subcategory:
                    found_subcategories.add(subcategory)
            
            print(f"   üìä Found {len(found_subcategories)} unique subcategories in database")
            
            # Check for new subcategories from canonical taxonomy
            new_subcategories = [
                "Partnerships", "Maxima and Minima", "Special Polynomials",
                "Mensuration 2D", "Mensuration 3D", "Number Properties", 
                "Number Series", "Factorials"
            ]
            
            found_new = []
            missing_new = []
            
            for subcat in new_subcategories:
                if subcat in found_subcategories:
                    found_new.append(subcat)
                else:
                    missing_new.append(subcat)
            
            print(f"   ‚úÖ Found new subcategories: {found_new}")
            if missing_new:
                print(f"   ‚ö†Ô∏è Missing new subcategories: {missing_new}")
            
            if len(found_new) > 0:
                taxonomy_results["new_subcategories_added"] = True
                print("   ‚úÖ New subcategories successfully added to database")
            else:
                print("   ‚ùå No new subcategories found - may need manual addition")
        
        # TEST 3: Question Classification Test
        print("\nüîç TEST 3: QUESTION CLASSIFICATION WITH NEW TAXONOMY")
        print("-" * 40)
        print("Testing question creation and classification with new taxonomy structure")
        
        # Test question for each new category
        test_questions = [
            {
                "stem": "TAXONOMY TEST: In a partnership, A invests Rs. 5000 and B invests Rs. 7000. If the profit is Rs. 2400, what is A's share?",
                "hint_category": "Arithmetic",
                "hint_subcategory": "Partnerships",
                "source": "Taxonomy Test"
            },
            {
                "stem": "TAXONOMY TEST: Find the maximum value of the function f(x) = -x¬≤ + 4x + 5.",
                "hint_category": "Algebra", 
                "hint_subcategory": "Maxima and Minima",
                "source": "Taxonomy Test"
            },
            {
                "stem": "TAXONOMY TEST: Find the area of a rectangle with length 12 cm and width 8 cm.",
                "hint_category": "Geometry and Mensuration",
                "hint_subcategory": "Mensuration 2D",
                "source": "Taxonomy Test"
            }
        ]
        
        created_questions = []
        for i, test_q in enumerate(test_questions):
            success, response = self.run_test(f"Create Test Question {i+1}", "POST", "questions", 200, test_q, headers)
            if success and 'question_id' in response:
                created_questions.append({
                    'id': response['question_id'],
                    'subcategory': test_q['hint_subcategory']
                })
                print(f"   ‚úÖ Created question for {test_q['hint_subcategory']}")
            else:
                print(f"   ‚ùå Failed to create question for {test_q['hint_subcategory']}")
        
        if len(created_questions) > 0:
            taxonomy_results["question_classification_test"] = True
            print(f"   ‚úÖ Successfully created {len(created_questions)} questions with new taxonomy")
        
        # TEST 4: LLM Enrichment with New Taxonomy
        print("\nü§ñ TEST 4: LLM ENRICHMENT WITH NEW TAXONOMY")
        print("-" * 40)
        print("Testing LLM enrichment works with updated taxonomy structure")
        
        if created_questions:
            # Wait for LLM processing
            print("   ‚è≥ Waiting 15 seconds for LLM enrichment...")
            time.sleep(15)
            
            # Check enrichment results
            success, response = self.run_test("Check LLM Enrichment Results", "GET", "questions?limit=50", 200, None, headers)
            if success:
                questions = response.get('questions', [])
                enriched_count = 0
                
                for created_q in created_questions:
                    for q in questions:
                        if q.get('id') == created_q['id']:
                            answer = q.get('answer', '')
                            solution = q.get('solution_approach', '')
                            
                            # Check if enriched (not generic)
                            if (answer and answer != "To be generated by LLM" and 
                                solution and "Mathematical approach" not in solution):
                                enriched_count += 1
                                print(f"   ‚úÖ Question enriched for {created_q['subcategory']}")
                            break
                
                if enriched_count > 0:
                    taxonomy_results["llm_enrichment_with_taxonomy"] = True
                    print(f"   ‚úÖ LLM enrichment working with new taxonomy ({enriched_count}/{len(created_questions)} questions)")
                else:
                    print("   ‚ö†Ô∏è LLM enrichment may need more time or has issues")
        
        # TEST 5: Session Creation with New Taxonomy
        print("\nüéØ TEST 5: SESSION CREATION WITH NEW TAXONOMY")
        print("-" * 40)
        print("Testing 12-question session creation works with updated taxonomy")
        
        # Login as student for session testing
        student_login = {
            "email": "student@catprep.com",
            "password": "student123"
        }
        
        success, response = self.run_test("Student Login for Session Test", "POST", "auth/login", 200, student_login)
        if success and 'access_token' in response:
            student_token = response['access_token']
            student_headers = {
                'Content-Type': 'application/json',
                'Authorization': f'Bearer {student_token}'
            }
            
            # Create session
            session_data = {"target_minutes": 30}
            success, response = self.run_test("Create Session with New Taxonomy", "POST", "sessions/start", 200, session_data, student_headers)
            if success and 'session_id' in response:
                session_id = response['session_id']
                session_type = response.get('session_type', 'unknown')
                total_questions = response.get('total_questions', 0)
                
                print(f"   ‚úÖ Session created: {session_id}")
                print(f"   üìä Session type: {session_type}")
                print(f"   üìä Total questions: {total_questions}")
                
                if total_questions >= 12:
                    taxonomy_results["session_creation_with_taxonomy"] = True
                    print("   ‚úÖ 12-question session working with new taxonomy")
                else:
                    print(f"   ‚ö†Ô∏è Session has only {total_questions} questions (expected 12)")
                
                # Test getting questions from session
                success, response = self.run_test("Get Question from Session", "GET", f"sessions/{session_id}/next-question", 200, None, student_headers)
                if success and 'question' in response:
                    question = response['question']
                    subcategory = question.get('subcategory', '')
                    print(f"   ‚úÖ Retrieved question with subcategory: {subcategory}")
                    
                    # Check if it's using new taxonomy
                    if subcategory in [item for sublist in self.canonical_taxonomy.values() for item in sublist]:
                        print("   ‚úÖ Question uses canonical taxonomy subcategory")
            else:
                print("   ‚ùå Failed to create session")
        else:
            print("   ‚ùå Failed to login as student")
        
        # TEST 6: Category Structure Validation
        print("\nüìä TEST 6: CATEGORY STRUCTURE VALIDATION")
        print("-" * 40)
        print("Validating new category structure (without A-, B-, C- prefixes)")
        
        success, response = self.run_test("Get Dashboard for Category Check", "GET", "dashboard/mastery", 200, None, headers)
        if success:
            mastery_data = response.get('mastery_by_topic', [])
            found_categories = set()
            
            for topic in mastery_data:
                category = topic.get('category_name', '')
                if category:
                    found_categories.add(category)
            
            print(f"   üìä Found categories: {list(found_categories)}")
            
            # Check for new category names (without prefixes)
            expected_categories = ["Arithmetic", "Algebra", "Geometry and Mensuration", "Number System", "Modern Math"]
            new_format_found = any(cat in found_categories for cat in expected_categories)
            
            if new_format_found:
                taxonomy_results["category_structure_validation"] = True
                print("   ‚úÖ New category structure detected")
            else:
                print("   ‚ö†Ô∏è Still using old category format (A-Arithmetic, B-Algebra, etc.)")
        
        # TEST 7: Subcategory Coverage Validation
        print("\nüìã TEST 7: SUBCATEGORY COVERAGE VALIDATION")
        print("-" * 40)
        print("Validating comprehensive subcategory coverage")
        
        success, response = self.run_test("Get Detailed Progress", "GET", "dashboard/mastery", 200, None, headers)
        if success:
            detailed_progress = response.get('detailed_progress', [])
            covered_subcategories = set()
            
            for progress in detailed_progress:
                subcategory = progress.get('subcategory', '')
                if subcategory:
                    covered_subcategories.add(subcategory)
            
            print(f"   üìä Total subcategories covered: {len(covered_subcategories)}")
            
            # Check coverage of canonical taxonomy
            all_canonical_subcategories = [item for sublist in self.canonical_taxonomy.values() for item in sublist]
            coverage_count = sum(1 for subcat in all_canonical_subcategories if subcat in covered_subcategories)
            coverage_percentage = (coverage_count / len(all_canonical_subcategories)) * 100
            
            print(f"   üìä Canonical taxonomy coverage: {coverage_count}/{len(all_canonical_subcategories)} ({coverage_percentage:.1f}%)")
            
            if coverage_percentage >= 50:
                taxonomy_results["subcategory_coverage_validation"] = True
                print("   ‚úÖ Good subcategory coverage of canonical taxonomy")
            else:
                print("   ‚ö†Ô∏è Low subcategory coverage - may need more questions")
        
        # FINAL RESULTS SUMMARY
        print("\n" + "=" * 60)
        print("CANONICAL TAXONOMY UPDATE RESULTS")
        print("=" * 60)
        
        passed_tests = sum(taxonomy_results.values())
        total_tests = len(taxonomy_results)
        success_rate = (passed_tests / total_tests) * 100
        
        for test_name, result in taxonomy_results.items():
            status = "‚úÖ PASS" if result else "‚ùå FAIL"
            print(f"{test_name.replace('_', ' ').title():<40} {status}")
            
        print("-" * 60)
        print(f"Overall Success Rate: {passed_tests}/{total_tests} ({success_rate:.1f}%)")
        
        # Critical analysis
        if taxonomy_results["topics_table_update"] and taxonomy_results["new_subcategories_added"]:
            print("üéâ TAXONOMY UPDATE SUCCESS: Database updated with new structure!")
        else:
            print("‚ùå TAXONOMY UPDATE ISSUES: Database may not be fully updated")
        
        if taxonomy_results["question_classification_test"] and taxonomy_results["llm_enrichment_with_taxonomy"]:
            print("‚úÖ QUESTION SYSTEM: Working with new taxonomy")
        else:
            print("‚ö†Ô∏è QUESTION SYSTEM: May have issues with new taxonomy")
        
        if taxonomy_results["session_creation_with_taxonomy"]:
            print("‚úÖ SESSION SYSTEM: 12-question sessions working with new taxonomy")
        else:
            print("‚ùå SESSION SYSTEM: Issues with new taxonomy structure")
        
        return success_rate >= 70

    def test_critical_llm_solution_re_enrichment(self):
        print("üö® CRITICAL LLM SOLUTION RE-ENRICHMENT TESTING")
        print("=" * 60)
        print("URGENT REQUIREMENT: Re-enrich ALL questions with generic/wrong solutions")
        print("Cannot leave any question in error state with generic solutions")
        print("Examples of generic solutions to find and fix:")
        print("- 'Mathematical approach to solve this problem'")
        print("- 'Example answer based on the question pattern'")
        print("- 'Detailed solution for: [question]...'")
        print("- 'To be generated by LLM'")
        print("Admin credentials: sumedhprabhu18@gmail.com / admin2025")
        print("=" * 60)
        
        # First authenticate as admin
        admin_login = {
            "email": "sumedhprabhu18@gmail.com",
            "password": "admin2025"
        }
        
        success, response = self.run_test("Admin Login", "POST", "auth/login", 200, admin_login)
        if not success or 'access_token' not in response:
            print("‚ùå Cannot test LLM re-enrichment - admin login failed")
            return False
            
        self.admin_token = response['access_token']
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {self.admin_token}'
        }
        
        re_enrichment_results = {
            "admin_authentication": True,
            "database_scan_for_generic_solutions": False,
            "generic_solutions_identified": False,
            "llm_enrichment_pipeline_test": False,
            "force_re_enrichment_test": False,
            "solution_quality_verification": False,
            "database_update_verification": False,
            "student_experience_protection": False
        }
        
        # TEST 1: Database Scan for Generic Solutions
        print("\nüîç TEST 1: DATABASE SCAN FOR GENERIC SOLUTIONS")
        print("-" * 40)
        print("Scanning all questions in database for generic/wrong solutions")
        
        success, response = self.run_test("Get All Questions for Generic Solution Scan", "GET", "questions?limit=300", 200, None, headers)
        if success:
            questions = response.get('questions', [])
            print(f"   ‚úÖ Retrieved {len(questions)} questions from database")
            re_enrichment_results["database_scan_for_generic_solutions"] = True
            
            # Define generic solution patterns to identify
            generic_patterns = [
                "mathematical approach to solve this problem",
                "example answer based on the question pattern",
                "detailed solution for:",
                "to be generated by llm",
                "approach to solve this problem",
                "solution based on the question pattern",
                "mathematical solution approach",
                "step-by-step solution approach"
            ]
            
            generic_questions = []
            wrong_solution_questions = []
            
            for q in questions:
                question_id = q.get('id')
                stem = (q.get('stem') or '').lower()
                answer = (q.get('answer') or '').lower()
                solution_approach = (q.get('solution_approach') or '').lower()
                detailed_solution = (q.get('detailed_solution') or '').lower()
                
                # Check for generic solutions
                is_generic = False
                for pattern in generic_patterns:
                    if (pattern in solution_approach or 
                        pattern in detailed_solution or 
                        pattern in answer):
                        is_generic = True
                        break
                
                if is_generic:
                    generic_questions.append({
                        'id': question_id,
                        'stem': q.get('stem', '')[:80] + '...',
                        'answer': q.get('answer', '')[:50] + '...',
                        'solution_approach': q.get('solution_approach', '')[:80] + '...',
                        'detailed_solution': q.get('detailed_solution', '')[:80] + '...'
                    })
                
                # Check for solution mismatches (e.g., salary question with alloy solution)
                if 'earn' in stem or 'salary' in stem or 'income' in stem:
                    if ('alloy' in solution_approach or 'copper' in solution_approach or 
                        'aluminum' in solution_approach or 'metal' in solution_approach):
                        wrong_solution_questions.append({
                            'id': question_id,
                            'stem': q.get('stem', '')[:80] + '...',
                            'solution_approach': q.get('solution_approach', '')[:80] + '...',
                            'mismatch_type': 'salary_question_with_alloy_solution'
                        })
            
            print(f"   üö® FOUND {len(generic_questions)} questions with GENERIC solutions")
            print(f"   üö® FOUND {len(wrong_solution_questions)} questions with WRONG solutions")
            
            if len(generic_questions) > 0 or len(wrong_solution_questions) > 0:
                re_enrichment_results["generic_solutions_identified"] = True
                
                print("\n   üìã GENERIC SOLUTION EXAMPLES:")
                for i, q in enumerate(generic_questions[:5]):
                    print(f"     {i+1}. ID: {q['id']}")
                    print(f"        Question: {q['stem']}")
                    print(f"        Generic Solution: {q['solution_approach']}")
                    print()
                
                if wrong_solution_questions:
                    print("   üìã WRONG SOLUTION EXAMPLES:")
                    for i, q in enumerate(wrong_solution_questions[:3]):
                        print(f"     {i+1}. ID: {q['id']}")
                        print(f"        Question: {q['stem']}")
                        print(f"        Wrong Solution: {q['solution_approach']}")
                        print(f"        Mismatch Type: {q['mismatch_type']}")
                        print()
            else:
                print("   ‚úÖ No generic or wrong solutions found in database")
                re_enrichment_results["solution_quality_verification"] = True
        else:
            print("   ‚ùå Failed to retrieve questions from database")
            return False
        
        # TEST 2: LLM Enrichment Pipeline Test
        print("\nü§ñ TEST 2: LLM ENRICHMENT PIPELINE TEST")
        print("-" * 40)
        print("Testing if LLM enrichment pipeline generates proper solutions")
        
        # Create a test question to verify LLM enrichment works correctly
        test_question_data = {
            "stem": "RE-ENRICHMENT TEST: A person earns 30% more than his colleague. If the colleague earns Rs. 2000, how much does the person earn?",
            "hint_category": "Arithmetic",
            "hint_subcategory": "Percentages",
            "source": "Re-enrichment Test"
        }
        
        success, response = self.run_test("Create Test Question for LLM Pipeline", "POST", "questions", 200, test_question_data, headers)
        if success and 'question_id' in response:
            test_question_id = response['question_id']
            print(f"   ‚úÖ Test question created: {test_question_id}")
            print(f"   Status: {response.get('status')}")
            
            # Wait for LLM processing
            print("   ‚è≥ Waiting 15 seconds for LLM enrichment...")
            time.sleep(15)
            
            # Check if LLM generated proper solution
            success, response = self.run_test("Check LLM Pipeline Results", "GET", f"questions?limit=50", 200, None, headers)
            if success:
                questions = response.get('questions', [])
                test_question = None
                
                for q in questions:
                    if q.get('id') == test_question_id:
                        test_question = q
                        break
                
                if test_question:
                    answer = test_question.get('answer', '')
                    solution_approach = test_question.get('solution_approach', '')
                    detailed_solution = test_question.get('detailed_solution', '')
                    
                    print(f"   Generated Answer: {answer}")
                    print(f"   Generated Solution Approach: {solution_approach}")
                    print(f"   Generated Detailed Solution: {detailed_solution[:100]}...")
                    
                    # Check if solution is relevant and not generic
                    is_relevant = False
                    is_not_generic = True
                    
                    if answer and answer != "To be generated by LLM":
                        # Check for relevant content (should mention percentage, earning, 2600, etc.)
                        if ('2600' in str(answer) or '30%' in solution_approach or 
                            'percentage' in solution_approach.lower() or 
                            'earn' in solution_approach.lower()):
                            is_relevant = True
                    
                    # Check it's not generic
                    for pattern in generic_patterns:
                        if pattern in solution_approach.lower() or pattern in detailed_solution.lower():
                            is_not_generic = False
                            break
                    
                    if is_relevant and is_not_generic:
                        print("   ‚úÖ LLM enrichment pipeline working correctly")
                        print("   ‚úÖ Generated relevant, non-generic solution")
                        re_enrichment_results["llm_enrichment_pipeline_test"] = True
                    else:
                        print("   ‚ùå LLM enrichment pipeline issues detected")
                        if not is_relevant:
                            print("   ‚ùå Solution not relevant to question")
                        if not is_not_generic:
                            print("   ‚ùå Solution is still generic")
                else:
                    print("   ‚ùå Test question not found after creation")
        else:
            print("   ‚ùå Failed to create test question for LLM pipeline test")
        
        # TEST 3: CRITICAL - Use Mass Re-enrichment API Endpoint
        print("\nüîÑ TEST 3: CRITICAL MASS RE-ENRICHMENT API ENDPOINT")
        print("-" * 40)
        print("Using /api/admin/re-enrich-all-questions endpoint to fix ALL generic solutions")
        
        if len(generic_questions) > 0:
            print(f"   üö® CRITICAL: Found {len(generic_questions)} questions with generic solutions")
            print("   üîÑ Calling mass re-enrichment API endpoint...")
            
            # Call the critical re-enrichment endpoint
            success, response = self.run_test("Mass Re-enrichment API Call", "POST", "admin/re-enrich-all-questions", 200, {}, headers)
            if success:
                processed = response.get('processed', 0)
                success_count = response.get('success', 0)
                failed_count = response.get('failed', 0)
                status = response.get('status', 'unknown')
                
                print(f"   ‚úÖ Mass re-enrichment API called successfully")
                print(f"   üìä Status: {status}")
                print(f"   üìä Questions processed: {processed}")
                print(f"   üìä Successfully re-enriched: {success_count}")
                print(f"   üìä Failed to re-enrich: {failed_count}")
                
                if success_count > 0:
                    re_enrichment_results["force_re_enrichment_test"] = True
                    print(f"   üéâ SUCCESS: {success_count} questions re-enriched with proper solutions!")
                    
                    # Wait for processing to complete
                    print("   ‚è≥ Waiting 30 seconds for mass re-enrichment to complete...")
                    time.sleep(30)
                else:
                    print(f"   ‚ö†Ô∏è WARNING: No questions were successfully re-enriched")
                    if failed_count > 0:
                        print(f"   ‚ùå {failed_count} questions failed re-enrichment")
            else:
                print("   ‚ùå CRITICAL FAILURE: Mass re-enrichment API call failed")
                print("   üö® This is a production-blocking issue!")
        else:
            print("   ‚úÖ No generic questions found - database already clean")
            re_enrichment_results["force_re_enrichment_test"] = True
        
        # TEST 4: Database Update Verification
        print("\n‚úÖ TEST 4: DATABASE UPDATE VERIFICATION")
        print("-" * 40)
        print("Verifying that re-enrichment updated the database correctly")
        
        # Re-scan database for generic solutions
        success, response = self.run_test("Re-scan Database After Re-enrichment", "GET", "questions?limit=300", 200, None, headers)
        if success:
            questions_after = response.get('questions', [])
            print(f"   ‚úÖ Retrieved {len(questions_after)} questions after re-enrichment")
            
            # Count generic solutions again
            generic_count_after = 0
            for q in questions_after:
                solution_approach = (q.get('solution_approach') or '').lower()
                detailed_solution = (q.get('detailed_solution') or '').lower()
                answer = (q.get('answer') or '').lower()
                
                for pattern in generic_patterns:
                    if (pattern in solution_approach or 
                        pattern in detailed_solution or 
                        pattern in answer):
                        generic_count_after += 1
                        break
            
            original_generic_count = len(generic_questions)
            print(f"   üìä Generic solutions before: {original_generic_count}")
            print(f"   üìä Generic solutions after: {generic_count_after}")
            
            if generic_count_after < original_generic_count:
                print("   ‚úÖ Database update successful - generic solutions reduced")
                re_enrichment_results["database_update_verification"] = True
            elif generic_count_after == 0:
                print("   üéâ PERFECT! No generic solutions remaining in database")
                re_enrichment_results["database_update_verification"] = True
                re_enrichment_results["student_experience_protection"] = True
            else:
                print("   ‚ö†Ô∏è Generic solutions still present - may need more time or manual intervention")
        else:
            print("   ‚ùå Failed to verify database updates")
        
        # TEST 5: Student Experience Protection Verification
        print("\nüë®‚Äçüéì TEST 5: STUDENT EXPERIENCE PROTECTION")
        print("-" * 40)
        print("Verifying students won't see generic/wrong solutions")
        
        # Test session creation and question retrieval to ensure students get proper solutions
        student_login = {
            "email": "student@catprep.com",
            "password": "student123"
        }
        
        success, response = self.run_test("Student Login for Experience Test", "POST", "auth/login", 200, student_login)
        if success and 'access_token' in response:
            student_token = response['access_token']
            student_headers = {
                'Content-Type': 'application/json',
                'Authorization': f'Bearer {student_token}'
            }
            
            # Create a session to test student experience
            session_data = {"target_minutes": 30}
            success, response = self.run_test("Create Student Session", "POST", "sessions/start", 200, session_data, student_headers)
            if success and 'session_id' in response:
                session_id = response['session_id']
                print(f"   ‚úÖ Student session created: {session_id}")
                
                # Get a question from the session
                success, response = self.run_test("Get Student Question", "GET", f"sessions/{session_id}/next-question", 200, None, student_headers)
                if success and 'question' in response:
                    question = response['question']
                    question_stem = question.get('stem', '')
                    
                    print(f"   ‚úÖ Student question retrieved")
                    print(f"   Question: {question_stem[:60]}...")
                    
                    # Submit an answer to get solution feedback
                    answer_data = {
                        "question_id": question['id'],
                        "user_answer": "test_answer",
                        "time_sec": 30,
                        "hint_used": False
                    }
                    
                    success, response = self.run_test("Submit Answer for Solution", "POST", f"sessions/{session_id}/submit-answer", 200, answer_data, student_headers)
                    if success and 'solution_feedback' in response:
                        solution_feedback = response['solution_feedback']
                        solution_approach = solution_feedback.get('solution_approach', '').lower()
                        detailed_solution = solution_feedback.get('detailed_solution', '').lower()
                        
                        print(f"   Solution shown to student: {solution_approach[:80]}...")
                        
                        # Check if student sees generic solution
                        student_sees_generic = False
                        for pattern in generic_patterns:
                            if pattern in solution_approach or pattern in detailed_solution:
                                student_sees_generic = True
                                break
                        
                        if not student_sees_generic:
                            print("   ‚úÖ Student protected - no generic solutions shown")
                            re_enrichment_results["student_experience_protection"] = True
                        else:
                            print("   ‚ùå CRITICAL: Student still seeing generic solutions!")
                    else:
                        print("   ‚ö†Ô∏è Could not verify solution feedback to student")
                else:
                    print("   ‚ö†Ô∏è No questions available in student session")
            else:
                print("   ‚ùå Failed to create student session")
        else:
            print("   ‚ùå Failed to login as student")
        
        # FINAL RESULTS SUMMARY
        print("\n" + "=" * 60)
        print("CRITICAL LLM SOLUTION RE-ENRICHMENT RESULTS")
        print("=" * 60)
        
        passed_tests = sum(re_enrichment_results.values())
        total_tests = len(re_enrichment_results)
        success_rate = (passed_tests / total_tests) * 100
        
        for test_name, result in re_enrichment_results.items():
            status = "‚úÖ PASS" if result else "‚ùå FAIL"
            print(f"{test_name.replace('_', ' ').title():<40} {status}")
            
        print("-" * 60)
        print(f"Overall Success Rate: {passed_tests}/{total_tests} ({success_rate:.1f}%)")
        
        # Critical analysis
        if re_enrichment_results["generic_solutions_identified"]:
            if re_enrichment_results["database_update_verification"]:
                print("üéâ CRITICAL SUCCESS: Generic solutions identified and fixed!")
                print("   ‚úÖ Database updated with proper solutions")
            else:
                print("‚ùå CRITICAL ISSUE: Generic solutions found but not fixed!")
                print("   üö® URGENT: Manual intervention required")
        else:
            print("‚úÖ EXCELLENT: No generic solutions found in database")
        
        if re_enrichment_results["student_experience_protection"]:
            print("‚úÖ STUDENT SAFETY: Students protected from generic solutions")
        else:
            print("‚ùå STUDENT RISK: Students may still see generic solutions")
        
        if success_rate >= 80:
            print("üéâ LLM SOLUTION RE-ENRICHMENT SUCCESSFUL!")
            print("   ‚úÖ Database cleaned, students protected")
        elif success_rate >= 60:
            print("‚ö†Ô∏è Re-enrichment mostly successful with minor issues")
        else:
            print("‚ùå CRITICAL FAILURE: Re-enrichment process needs immediate attention")
            
        return success_rate >= 70

    def test_taxonomy_triple_with_8_unique_types(self):
        """Test taxonomy triple implementation with 8 unique Types for Type-based session generation"""
        print("üéØ TAXONOMY TRIPLE WITH 8 UNIQUE TYPES TESTING")
        print("=" * 60)
        print("Testing current taxonomy triple implementation with 8 unique Types to verify Type-based session generation works")
        print("UPDATED TESTING FOCUS:")
        print("- ‚úÖ 1126 questions with Type field populated (100% coverage)")
        print("- ‚úÖ 8 unique canonical Types assigned")
        print("- ‚úÖ Type field included in API responses")
        print("PRIMARY TEST OBJECTIVES:")
        print("1. Verify 12-Question Session Generation (not 2)")
        print("2. Check session uses Type diversity from available 8 Types")
        print("3. Verify Type-aware selection at (Category, Subcategory, Type) granularity")
        print("4. Test Type diversity enforcement with 8 available Types")
        print("5. Verify category mapping: Time-Speed-Distance ‚Üí Arithmetic")
        print("6. Check session metadata includes Type tracking fields")
        print("Admin credentials: sumedhprabhu18@gmail.com / admin2025")
        print("=" * 60)
        
        # First authenticate as admin
        admin_login = {
            "email": "sumedhprabhu18@gmail.com",
            "password": "admin2025"
        }
        
        success, response = self.run_test("Admin Login", "POST", "auth/login", 200, admin_login)
        if not success or 'access_token' not in response:
            print("‚ùå Cannot test Type-based sessions - admin login failed")
            return False
            
        self.admin_token = response['access_token']
        admin_headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {self.admin_token}'
        }
        
        # Login as student for session testing
        student_login = {
            "email": "student@catprep.com",
            "password": "student123"
        }
        
        success, response = self.run_test("Student Login", "POST", "auth/login", 200, student_login)
        if not success or 'access_token' not in response:
            print("‚ùå Cannot test sessions - student login failed")
            return False
            
        student_token = response['access_token']
        student_headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {student_token}'
        }
        
        # Expected 8 canonical Types from review request
        expected_8_types = [
            "Basics", "Trains", "Circular Track Motion", "Races", 
            "Relative Speed", "Boats and Streams", "Two variable systems", 
            "Work Time Efficiency"
        ]
        
        type_results = {
            "admin_authentication": True,
            "student_authentication": True,
            "type_field_api_verification": False,
            "eight_unique_types_verification": False,
            "twelve_question_session_generation": False,
            "type_diversity_enforcement": False,
            "category_mapping_verification": False,
            "session_metadata_type_tracking": False,
            "type_aware_pyq_weighting": False,
            "session_intelligence_type_rationale": False
        }
        
        # TEST 1: Verify Type Field in API Response
        print("\nüîç TEST 1: VERIFY TYPE FIELD IN API RESPONSE")
        print("-" * 40)
        print("Testing /api/questions endpoint to confirm it returns type_of_question field")
        print("Expected: 1126 questions with Type field populated (100% coverage)")
        
        success, response = self.run_test("Get Questions with Type Field Check", "GET", "questions?limit=1200", 200, None, admin_headers)
        if success:
            questions = response.get('questions', [])
            print(f"   üìä Total questions retrieved: {len(questions)}")
            
            if questions:
                # Check Type field presence and population
                questions_with_type = 0
                unique_types = set()
                
                for q in questions:
                    has_type_field = 'type_of_question' in q
                    type_value = q.get('type_of_question', '')
                    
                    if has_type_field and type_value and type_value.strip():
                        questions_with_type += 1
                        unique_types.add(type_value)
                
                type_coverage = (questions_with_type / len(questions)) * 100 if questions else 0
                
                print(f"   üìä Questions with type_of_question field: {questions_with_type}/{len(questions)} ({type_coverage:.1f}%)")
                print(f"   üìä Unique Types found: {len(unique_types)}")
                print(f"   üìä All Types: {sorted(list(unique_types))}")
                
                if type_coverage >= 99 and questions_with_type >= 1100:
                    type_results["type_field_api_verification"] = True
                    print("   ‚úÖ Type field properly populated in API responses")
                    print(f"   ‚úÖ Achieved {type_coverage:.1f}% coverage (expected 100%)")
                else:
                    print("   ‚ùå Type field coverage insufficient")
                    print(f"   ‚ùå Expected 1126 questions with Type field, found {questions_with_type}")
            else:
                print("   ‚ùå No questions found for Type field verification")
        
        # TEST 2: Database Coverage Validation
        print("\nüìä TEST 2: DATABASE COVERAGE VALIDATION")
        print("-" * 40)
        print("Confirming database migration results: 1126/1126 questions with Type field populated")
        print("Testing that category mapping works for Time-Speed-Distance (1099 questions) ‚Üí Arithmetic")
        print("Verifying canonical taxonomy compliance (99.2% expected)")
        
        success, response = self.run_test("Get All Questions for Migration Validation", "GET", "questions?limit=1200", 200, None, admin_headers)
        if success:
            questions = response.get('questions', [])
            print(f"   üìä Total questions in database: {len(questions)}")
            
            # Analyze migration results
            questions_with_type = 0
            tsd_questions = 0
            arithmetic_questions = 0
            canonical_questions = 0
            types_found = set()
            subcategories_found = set()
            
            for q in questions:
                question_type = q.get('type_of_question', '')
                subcategory = q.get('subcategory', '')
                
                # Count questions with Type field
                if question_type and question_type.strip():
                    questions_with_type += 1
                    types_found.add(question_type)
                
                # Count Time-Speed-Distance questions
                if 'time' in subcategory.lower() and 'speed' in subcategory.lower():
                    tsd_questions += 1
                
                # Count Arithmetic category questions
                if any(arith_sub in subcategory for arith_sub in self.canonical_taxonomy.get('Arithmetic', [])):
                    arithmetic_questions += 1
                
                # Count canonical taxonomy compliance
                for category, subcats in self.canonical_taxonomy.items():
                    if subcategory in subcats:
                        canonical_questions += 1
                        break
                
                if subcategory:
                    subcategories_found.add(subcategory)
            
            # Calculate coverage percentages
            type_coverage = (questions_with_type / len(questions)) * 100 if questions else 0
            canonical_compliance = (canonical_questions / len(questions)) * 100 if questions else 0
            
            print(f"   üìä Questions with Type field: {questions_with_type}/{len(questions)} ({type_coverage:.1f}%)")
            print(f"   üìä Time-Speed-Distance questions: {tsd_questions}")
            print(f"   üìä Arithmetic category questions: {arithmetic_questions}")
            print(f"   üìä Canonical taxonomy compliance: {canonical_questions}/{len(questions)} ({canonical_compliance:.1f}%)")
            print(f"   üìä Unique Types found: {len(types_found)}")
            print(f"   üìä Unique Subcategories: {len(subcategories_found)}")
            
            # Expected results validation
            expected_total = 1126
            expected_compliance = 99.2
            expected_types = 129
            
            print(f"   üéØ Expected: {expected_total} questions with Type field")
            print(f"   üéØ Expected: {expected_compliance}% canonical compliance")
            print(f"   üéØ Expected: {expected_types} total canonical Types")
            
            if (questions_with_type >= expected_total * 0.9 and 
                canonical_compliance >= expected_compliance * 0.9 and
                len(types_found) >= expected_types * 0.5):
                type_results["canonical_taxonomy_coverage"] = True
                print("   ‚úÖ Database migration successful - meets expected coverage")
            else:
                print("   ‚ùå Database migration incomplete or failed")
                print(f"   ‚ùå Type coverage: {type_coverage:.1f}% (expected ~100%)")
                print(f"   ‚ùå Canonical compliance: {canonical_compliance:.1f}% (expected {expected_compliance}%)")
                print(f"   ‚ùå Type diversity: {len(types_found)} (expected {expected_types})")
        
        # TEST 3: Test 12-Question Session Generation
        print("\nüéØ TEST 3: TEST 12-QUESTION SESSION GENERATION")
        print("-" * 40)
        print("Testing /api/sessions/start endpoint to verify it generates 12 questions (not 2)")
        print("Checking that sessions operate at (Category, Subcategory, Type) granularity")
        print("Verifying session metadata includes Type diversity tracking")
        
        session_data = {"target_minutes": 30}
        success, response = self.run_test("Create 12-Question Session", "POST", "sessions/start", 200, session_data, student_headers)
        if success:
            session_id = response.get('session_id')
            session_type = response.get('session_type')
            total_questions = response.get('total_questions', 0)
            personalization = response.get('personalization', {})
            
            print(f"   ‚úÖ Session created: {session_id}")
            print(f"   üìä Session type: {session_type}")
            print(f"   üìä Total questions: {total_questions}")
            print(f"   üìä Personalization applied: {personalization.get('applied', False)}")
            
            # Check session metadata for Type-related fields
            type_diversity = personalization.get('type_diversity', {})
            type_distribution = personalization.get('type_distribution', {})
            category_type_distribution = personalization.get('category_type_distribution', {})
            
            print(f"   üìä Type diversity: {type_diversity}")
            print(f"   üìä Type distribution: {type_distribution}")
            print(f"   üìä Category-Type distribution: {category_type_distribution}")
            
            # Validate 12-question requirement
            if total_questions == 12:
                print("   ‚úÖ CRITICAL SUCCESS: Session generates exactly 12 questions")
                type_results["type_based_session_creation"] = True
                self.session_id = session_id
            elif total_questions == 2:
                print("   ‚ùå CRITICAL FAILURE: Session generates only 2 questions (expected 12)")
                print("   ‚ùå This indicates Type-based selection logic is not working")
            else:
                print(f"   ‚ö†Ô∏è Unexpected question count: {total_questions} (expected 12)")
                if total_questions >= 10:
                    type_results["type_based_session_creation"] = True
                    self.session_id = session_id
        else:
            print("   ‚ùå Failed to create session")
        
        # TEST 4: Type-Based Selection Verification
        print("\nüîÑ TEST 4: TYPE-BASED SELECTION VERIFICATION")
        print("-" * 40)
        print("Verifying Type diversity enforcement (max 2 questions per Type, minimum 8 different Types per session)")
        print("Testing that Type-aware PYQ weighting works in question selection")
        print("Checking that sessions use canonical taxonomy categories mapping")
        
        if hasattr(self, 'session_id') and self.session_id:
            # Get all questions from session to analyze Type diversity
            type_distribution = {}
            subcategory_distribution = {}
            questions_with_types = 0
            questions_analyzed = 0
            pyq_scores_found = []
            
            # Analyze up to 12 questions from the session
            for i in range(12):
                success, response = self.run_test(f"Get Question {i+1} for Type Analysis", "GET", f"sessions/{self.session_id}/next-question", 200, None, student_headers)
                if success and 'question' in response:
                    question = response['question']
                    question_type = question.get('type_of_question', '')
                    subcategory = question.get('subcategory', '')
                    
                    questions_analyzed += 1
                    
                    if question_type and question_type.strip():
                        questions_with_types += 1
                        type_distribution[question_type] = type_distribution.get(question_type, 0) + 1
                    
                    if subcategory:
                        subcategory_distribution[subcategory] = subcategory_distribution.get(subcategory, 0) + 1
                    
                    print(f"   üìä Q{i+1}: {subcategory} :: {question_type}")
                    
                    # Submit a dummy answer to proceed
                    answer_data = {
                        "question_id": question['id'],
                        "user_answer": "A",
                        "time_sec": 30,
                        "hint_used": False
                    }
                    self.run_test(f"Submit Answer Q{i+1}", "POST", f"sessions/{self.session_id}/submit-answer", 200, answer_data, student_headers)
                elif success and response.get('session_complete'):
                    print(f"   ‚úÖ Session completed after {i} questions")
                    break
                else:
                    print(f"   ‚ö†Ô∏è Could not get question {i+1}")
                    break
            
            print(f"   üìä Questions analyzed: {questions_analyzed}")
            print(f"   üìä Questions with Types: {questions_with_types}")
            print(f"   üìä Type distribution: {type_distribution}")
            print(f"   üìä Subcategory distribution: {subcategory_distribution}")
            
            # Analyze Type diversity enforcement
            unique_types = len(type_distribution)
            max_per_type = max(type_distribution.values()) if type_distribution else 0
            min_types_required = 8
            max_per_type_allowed = 2
            
            print(f"   üìä Unique Types found: {unique_types} (minimum required: {min_types_required})")
            print(f"   üìä Max questions per Type: {max_per_type} (maximum allowed: {max_per_type_allowed})")
            
            # Check Type diversity requirements from review request
            type_diversity_ok = unique_types >= min_types_required and max_per_type <= max_per_type_allowed
            type_coverage_ok = (questions_with_types / questions_analyzed) >= 0.8 if questions_analyzed > 0 else False
            
            if type_diversity_ok and type_coverage_ok:
                type_results["type_diversity_enforcement"] = True
                print("   ‚úÖ Type diversity enforcement working correctly")
                print(f"   ‚úÖ Meets requirement: minimum {min_types_required} different Types per session")
                print(f"   ‚úÖ Meets requirement: max {max_per_type_allowed} questions per Type")
            else:
                print("   ‚ùå Type diversity enforcement not working")
                if not type_diversity_ok:
                    print(f"   ‚ùå Type diversity insufficient: {unique_types} types, max {max_per_type} per type")
                if not type_coverage_ok:
                    print(f"   ‚ùå Type coverage insufficient: {questions_with_types}/{questions_analyzed} questions have Types")
        else:
            print("   ‚ùå No session available for Type diversity testing")
        
        # TEST 5: Type Metadata Tracking
        print("\nüìä TEST 5: TYPE METADATA TRACKING")
        print("-" * 40)
        print("Verifying session metadata includes Type diversity tracking")
        
        if hasattr(self, 'session_id') and self.session_id:
            # Check if session intelligence provides Type-based rationale
            success, response = self.run_test("Get Session Question with Intelligence", "GET", f"sessions/{self.session_id}/next-question", 200, None, student_headers)
            if success:
                session_intelligence = response.get('session_intelligence', {})
                session_progress = response.get('session_progress', {})
                
                print(f"   üìä Session intelligence: {session_intelligence}")
                print(f"   üìä Session progress: {session_progress}")
                
                has_type_rationale = any('type' in str(v).lower() for v in session_intelligence.values())
                has_category_focus = 'category_focus' in session_intelligence
                
                if has_type_rationale or has_category_focus:
                    type_results["type_metadata_tracking"] = True
                    print("   ‚úÖ Type metadata tracking detected")
                else:
                    print("   ‚ö†Ô∏è Limited Type metadata in session intelligence")
        
        # TEST 6: PYQ Type Integration
        print("\nüîç TEST 6: PYQ TYPE INTEGRATION")
        print("-" * 40)
        print("Verifying PYQ frequency weighting considers Type dimension")
        
        success, response = self.run_test("Check PYQ Integration", "GET", "questions?limit=50", 200, None, admin_headers)
        if success:
            questions = response.get('questions', [])
            pyq_questions_with_types = 0
            pyq_frequency_scores = []
            
            for q in questions:
                pyq_score = q.get('pyq_frequency_score')
                question_type = q.get('type_of_question')
                
                if pyq_score is not None and question_type:
                    pyq_questions_with_types += 1
                    pyq_frequency_scores.append(float(pyq_score))
            
            print(f"   üìä Questions with PYQ scores and Types: {pyq_questions_with_types}")
            if pyq_frequency_scores:
                avg_pyq_score = sum(pyq_frequency_scores) / len(pyq_frequency_scores)
                print(f"   üìä Average PYQ frequency score: {avg_pyq_score:.3f}")
                
                if pyq_questions_with_types >= 10 and avg_pyq_score > 0.1:
                    type_results["pyq_type_integration"] = True
                    print("   ‚úÖ PYQ Type integration working")
                else:
                    print("   ‚ö†Ô∏è Limited PYQ Type integration")
        
        # TEST 7: Session Intelligence Type Rationale
        print("\nü§ñ TEST 7: SESSION INTELLIGENCE TYPE RATIONALE")
        print("-" * 40)
        print("Testing session intelligence provides Type-based rationale")
        
        # Create a fresh session to test intelligence
        session_data = {"target_minutes": 30}
        success, response = self.run_test("Create Fresh Session for Intelligence Test", "POST", "sessions/start", 200, session_data, student_headers)
        if success:
            fresh_session_id = response.get('session_id')
            personalization = response.get('personalization', {})
            
            print(f"   üìä Fresh session personalization: {personalization}")
            
            # Check for Type-related metadata
            type_distribution = personalization.get('type_distribution', {})
            category_type_distribution = personalization.get('category_type_distribution', {})
            
            print(f"   üìä Type distribution in metadata: {type_distribution}")
            print(f"   üìä Category-Type distribution: {len(category_type_distribution)} combinations")
            
            if type_distribution or category_type_distribution:
                type_results["session_intelligence_type_rationale"] = True
                print("   ‚úÖ Session intelligence includes Type-based rationale")
            else:
                print("   ‚ö†Ô∏è Limited Type-based intelligence in session metadata")
        
        # FINAL RESULTS SUMMARY
        print("\n" + "=" * 60)
        print("TYPE-BASED SESSION GENERATION RESULTS")
        print("=" * 60)
        
        passed_tests = sum(type_results.values())
        total_tests = len(type_results)
        success_rate = (passed_tests / total_tests) * 100
        
        for test_name, result in type_results.items():
            status = "‚úÖ PASS" if result else "‚ùå FAIL"
            print(f"{test_name.replace('_', ' ').title():<40} {status}")
            
        print("-" * 60)
        print(f"Overall Success Rate: {passed_tests}/{total_tests} ({success_rate:.1f}%)")
        
        # Critical analysis
        if type_results["database_schema_verification"] and type_results["canonical_taxonomy_coverage"]:
            print("üéâ DATABASE FOUNDATION: Ready for Type-based operations!")
        else:
            print("‚ùå DATABASE ISSUES: Schema or taxonomy coverage problems")
        
        if type_results["type_based_session_creation"] and type_results["type_diversity_enforcement"]:
            print("‚úÖ SESSION ENGINE: Type-based session generation working")
        else:
            print("‚ö†Ô∏è SESSION ENGINE: Issues with Type-based functionality")
        
        if type_results["pyq_type_integration"] and type_results["session_intelligence_type_rationale"]:
            print("‚úÖ INTELLIGENCE: Type-aware PYQ weighting and rationale working")
        else:
            print("‚ùå INTELLIGENCE: Limited Type-aware functionality")
        
        return success_rate >= 70

def main():
    """Main test execution"""
    print("üöÄ STARTING TYPE-BASED SESSION GENERATION TESTING")
    print("=" * 60)
    
    tester = CATBackendTester()
    
    # Test the complete taxonomy triple implementation
    print("TESTING: Complete Taxonomy Triple (Category, Subcategory, Type) Implementation")
    type_success = tester.test_complete_taxonomy_triple_implementation()
    
    print("\n" + "=" * 60)
    print("FINAL TEST SUMMARY")
    print("=" * 60)
    
    if type_success:
        print("üéâ TYPE-BASED SESSION GENERATION: SUCCESSFUL")
        print("   ‚úÖ Database schema supports taxonomy triple")
        print("   ‚úÖ Canonical taxonomy coverage verified")
        print("   ‚úÖ Type-based session generation working")
        print("   ‚úÖ Type diversity enforcement operational")
        print("   ‚úÖ Type metadata tracking functional")
        print("   ‚úÖ PYQ Type integration working")
        print("   ‚úÖ Session intelligence provides Type-based rationale")
    else:
        print("‚ùå TYPE-BASED SESSION GENERATION: ISSUES DETECTED")
        print("   üö® URGENT ACTION REQUIRED")
        print("   ‚ùå Some Type-based functionality not working properly")
        print("   ‚ùå May need further development or debugging")
    
    return type_success

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)