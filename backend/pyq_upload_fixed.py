async def upload_pyq_csv_fixed(file: UploadFile, db: AsyncSession, current_user: User):
    """
    FIXED: CSV-based PYQ upload with NO year dependency
    CSV columns: stem, image_url (both optional except stem)
    """
    try:
        # Read and validate CSV
        content = await file.read()
        content_str = content.decode('utf-8')
        
        # Parse CSV
        csv_reader = csv.DictReader(StringIO(content_str))
        csv_rows = list(csv_reader)
        
        # Validate CSV format - only stem is required
        if not csv_rows:
            raise HTTPException(status_code=400, detail="CSV file is empty")
        
        first_row = csv_rows[0]
        required_columns = ['stem']
        missing_columns = [col for col in required_columns if col not in first_row]
        if missing_columns:
            raise HTTPException(
                status_code=400, 
                detail=f"CSV must contain required columns: {', '.join(missing_columns)}. Found columns: {', '.join(first_row.keys())}"
            )
        
        # Process images from Google Drive URLs (same as questions)
        processed_rows = GoogleDriveImageFetcher.process_csv_image_urls(csv_rows, UPLOAD_DIR)
        
        # Create single PYQ ingestion record (no year)
        ingestion = PYQIngestion(
            upload_filename=file.filename,
            storage_key=f"pyq_csv_{uuid.uuid4()}.csv",
            year=None,  # No year dependency
            slot="CSV",
            source_url=None,
            pages_count=len(processed_rows),
            ocr_required=False,
            ocr_status="not_needed",
            parse_status="completed"
        )
        db.add(ingestion)
        await db.flush()
        
        # Create single PYQ paper record (no year)
        paper = PYQPaper(
            year=None,  # No year dependency
            slot="CSV",
            source_url=None,
            ingestion_id=str(ingestion.id)
        )
        db.add(paper)
        await db.flush()
        
        total_questions_created = 0
        total_images_processed = 0
        
        # Process all questions under single paper (no year grouping)
        for i, row in enumerate(processed_rows):
            stem = row.get('stem', '').strip()
            if not stem:
                logger.warning(f"Skipping row {i+1}: empty stem")
                continue
            
            if len(processed_rows) > 0 and 'image_url' in row and row['image_url']:
                total_images_processed += 1
            
            # Create a default topic for organization (will be updated by LLM)
            topic_result = await db.execute(
                select(Topic).where(Topic.name == "PYQ General")
            )
            topic = topic_result.scalar_one_or_none()
            
            if not topic:
                topic = Topic(
                    name="PYQ General",
                    category="A"
                )
                db.add(topic)
                await db.flush()
            
            # Create PYQ question with minimal data - LLM will enrich everything
            pyq_question = PYQQuestion(
                paper_id=str(paper.id),
                topic_id=str(topic.id),
                stem=stem,
                answer="To be generated by LLM",
                subcategory="To be classified by LLM",
                type_of_question="To be classified by LLM",
                tags=json.dumps(["pyq_csv_upload", "llm_pending"]),  # No year in tags
                confirmed=False  # Will be confirmed after LLM enrichment
            )
            
            db.add(pyq_question)
            total_questions_created += 1
        
        # Mark ingestion as completed
        ingestion.parse_status = "completed"
        ingestion.completed_at = datetime.utcnow()
        
        await db.commit()
        
        # Queue background LLM enrichment for all PYQ questions
        logger.info("Starting background LLM enrichment for all uploaded PYQ questions...")
        
        # Get all PYQ questions created in this batch for enrichment
        recent_pyq_questions = await db.execute(
            select(PYQQuestion).where(PYQQuestion.paper_id == str(paper.id))
        )
        
        for pyq_question in recent_pyq_questions.scalars():
            # Use the NEW enhanced enrichment pipeline instead of basic classification
            asyncio.create_task(enhanced_pyq_enrichment_background(str(pyq_question.id)))
        
        # Store file metadata for tracking (no year references)
        from database import PYQFiles
        
        file_record = PYQFiles(
            filename=file.filename,
            year=None,  # No year needed
            upload_date=datetime.utcnow(),
            processing_status="completed",
            file_size=len(content),
            storage_path=f"pyq_uploads/{file.filename}",
            file_metadata=json.dumps({
                "questions_created": total_questions_created,
                "images_processed": total_images_processed,
                "papers_created": 1,  # Always single paper
                "csv_rows_processed": len(processed_rows),
                "upload_timestamp": datetime.utcnow().isoformat(),
                "uploaded_by": current_user.email
            })
        )
        
        db.add(file_record)
        await db.commit()
        
        logger.info(f"PYQ CSV upload completed: {total_questions_created} questions created in single paper")
        
        return {
            "message": f"Successfully uploaded {total_questions_created} PYQ questions from CSV",
            "questions_created": total_questions_created,
            "images_processed": total_images_processed,
            "papers_created": 1,  # Always single paper
            "csv_rows_processed": len(processed_rows),
            "enrichment_status": "PYQ questions queued for automatic LLM processing (category classification, solution generation, type identification)",
            "note": "PYQ questions will be automatically enriched with categories, subcategories, question types, and solutions by the LLM system",
            "file_id": file_record.id
        }
        
    except Exception as e:
        logger.error(f"PYQ CSV upload error: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to upload PYQ CSV: {str(e)}")