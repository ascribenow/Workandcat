"""
Post-Session Summarizer Service
Analyzes user attempt history to extract concept dominance and readiness patterns
"""

import json
import logging
import time
from typing import Dict, Any, List
from database import SessionLocal
from sqlalchemy import text
from util.schemas import SUMMARIZER_SCHEMA
from util.llm_guarded import call_llm_json_with_retry
from services.deterministic_kernels import stable_semantic_id

logger = logging.getLogger(__name__)

class SummarizerService:
    """Post-session qualitative analysis using LLM"""
    
    def __init__(self):
        self.system_prompt = self._load_system_prompt()
    
    def _load_system_prompt(self) -> str:
        """Load summarizer system prompt"""
        return """You are an expert at analyzing student performance in CAT preparation to identify concept mastery patterns.

TASK: Analyze the student's attempt history and provide qualitative insights about concept dominance, readiness, and coverage patterns.

OUTPUT REQUIREMENTS:
1. **Concept Alias Mapping**: Normalize raw concept strings, reuse existing aliases when possible, only create provisional new concepts when necessary
2. **Dominance Analysis**: For each question attempt, identify 1-2 dominant concepts that drove the solution approach
3. **Readiness Assessment**: Evaluate concept mastery based on performance patterns (skipped, wrong streaks, correct patterns)
4. **Coverage Debt**: Identify concept pairs that need more practice based on recency and exposure

NORMALIZATION RULES:
- Reuse existing alias when ANY normalized member matches
- Only create "provisional": true new concepts when absolutely necessary
- Maintain consistency in concept identification

DOMINANCE RULES:
- High confidence + 1 dominant → 0.7/0.3 weight split
- High confidence + 2 dominants → 0.5/0.5 weight split
- Otherwise → equal split 1/k

READINESS CRITERIA:
- "skipped": High skip rate indicates avoidance
- "forever_wrong": Never got correct, indicates fundamental gap
- "wrong_1_to_3": Recent wrong streak, needs reinforcement
- "wrong_gt_3": Extended wrong streak, serious difficulty
- "cold_start": New concept, needs foundation building

Return ONLY valid JSON matching the required schema."""

    async def run(self, user_id: str, session_id: str = None, sess_seq: int = None) -> Dict[str, Any]:
        """
        Run post-session analysis for a user
        
        Args:
            user_id: User identifier  
            session_id: Session UUID (optional, will resolve to sess_seq)
            sess_seq: Session sequence number (optional)
            
        Returns:
            Summarizer analysis results
        """
        from services.telemetry import telemetry_service as telemetry
        from database import SessionLocal
        
        logger.info(f"🧠 Running Summarizer for user {user_id[:8]}...")
        
        db = SessionLocal()
        try:
            # 1) Resolve sess_seq from sessions if not given
            if sess_seq is None:
                row = db.execute(text("""
                    SELECT sess_seq
                    FROM sessions
                    WHERE user_id = :user_id AND session_id = :session_id
                    LIMIT 1
                """), {"user_id": user_id, "session_id": session_id}).fetchone()
                if not row:
                    telemetry.emit("summarizer_missing_session", {"user_id": user_id, "session_id": session_id})
                    return self._generate_empty_summary()  # no-op: don't fail pipeline
                sess_seq = row["sess_seq"]

            # 2) Fetch attempts by (user_id, sess_seq_at_serve)
            attempts = db.execute(text("""
                SELECT *
                FROM attempt_events
                WHERE user_id = :user_id AND sess_seq_at_serve = :sess_seq
                ORDER BY served_at ASC, attempt_id ASC
            """), {"user_id": user_id, "sess_seq": sess_seq}).fetchall()

            if not attempts:
                telemetry.emit("summarizer_no_attempts", {"user_id": user_id, "sess_seq": sess_seq, "session_id": session_id})
                return self._generate_empty_summary()  # nothing to summarize

            # Build payload with attempt history for this session
            payload = self._build_summarizer_payload_from_attempts(user_id, attempts)
            
            # Call LLM with schema validation and retry
            start_time = time.time()
            tokens_used = 0
            
            try:
                data = call_llm_json_with_retry(
                    system_prompt=self.system_prompt,
                    user_payload=payload,
                    schema=SUMMARIZER_SCHEMA,
                    model_primary="gpt-4o-mini",
                    model_fallback="gemini-1.5-pro",
                    max_retries=1
                )
                
                # Calculate telemetry
                elapsed_time = time.time() - start_time
                tokens_used = len(json.dumps(payload)) // 4  # Rough token estimate
                
                # Add telemetry to response
                data.setdefault("telemetry", {}).update({
                    "processing_time_ms": int(elapsed_time * 1000),
                    "estimated_tokens": tokens_used,
                    "alias_reuse_rate": self._calculate_alias_reuse_rate(data),
                    "provisional_new_count": self._count_provisional_concepts(data),
                    "llm_model_used": "gpt-4o-mini"
                })
                
                # 3) Persist session summary (use the UUID session_id you received)
                db.execute(text("""
                    INSERT INTO session_summary_llm
                        (session_id, user_id,
                         concept_alias_map, dominance, readiness_reasons, coverage_labels,
                         llm_model_used, created_at)
                    VALUES
                        (:session_id, :user_id, :concept_alias_map_json::jsonb, :dominance_json::jsonb, 
                         :readiness_reasons_json::jsonb, :coverage_labels_json::jsonb, :llm_model_used, NOW())
                """), {
                    "session_id": session_id,
                    "user_id": user_id,
                    "concept_alias_map_json": json.dumps(data.get("concept_alias_map_updated", [])),
                    "dominance_json": json.dumps(data.get("dominance_by_item", {})),
                    "readiness_reasons_json": json.dumps(data.get("concept_readiness_labels", [])),
                    "coverage_labels_json": json.dumps(data.get("pair_coverage_labels", [])),
                    "llm_model_used": data["telemetry"]["llm_model_used"]
                })

                # 4) Upsert per-user alias map
                if data.get("concept_alias_map_updated"):
                    db.execute(text("""
                        INSERT INTO concept_alias_map_latest (user_id, alias_map_json, updated_at)
                        VALUES (:user_id, :alias_map_json::jsonb, NOW())
                        ON CONFLICT (user_id) DO UPDATE
                          SET alias_map_json = EXCLUDED.alias_map_json,
                              updated_at = EXCLUDED.updated_at
                    """), {
                        "user_id": user_id, 
                        "alias_map_json": json.dumps(data.get("concept_alias_map_updated", []))
                    })

                db.commit()
                
                telemetry.emit("summarizer_ok", {
                    "user_id": user_id, "session_id": session_id, "sess_seq": sess_seq,
                    "llm_model_used": data["telemetry"]["llm_model_used"]
                })
                
                logger.info(f"✅ Summarizer completed for user {user_id[:8]} ({elapsed_time:.2f}s, ~{tokens_used} tokens)")
                return data
                
            except Exception as e:
                db.rollback()
                logger.error(f"❌ Summarizer failed for user {user_id[:8]}: {e}")
                return self._generate_empty_summary()
                
        finally:
            db.close()
    
    def _build_summarizer_payload_from_attempts(self, user_id: str, attempts: List) -> Dict[str, Any]:
        """Build payload for summarizer LLM call from attempt records"""
        # Format attempt history
        attempt_history = []
        for attempt in attempts:
            # Parse core concepts
            try:
                if isinstance(attempt.core_concepts, list):
                    concepts = attempt.core_concepts
                elif isinstance(attempt.core_concepts, str):
                    concepts = json.loads(attempt.core_concepts)
                else:
                    concepts = []
            except:
                concepts = []
            
            attempt_history.append({
                "question_id": attempt.question_id,
                "was_correct": attempt.was_correct,
                "skipped": attempt.skipped,
                "response_time_ms": attempt.response_time_ms,
                "sess_seq": attempt.sess_seq_at_serve,
                "difficulty_band": attempt.difficulty_band,
                "pair": f"{attempt.subcategory}:{attempt.type_of_question}",
                "core_concepts": concepts,
                "pyq_frequency_score": float(attempt.pyq_frequency_score) if attempt.pyq_frequency_score else 0.5
            })
        
        # Get existing concept alias map (if any)
        existing_aliases = self._get_existing_concept_aliases(user_id)
        
        return {
            "user_id": user_id,
            "attempt_history": attempt_history,
            "existing_concept_aliases": existing_aliases,
            "analysis_instructions": {
                "focus_areas": [
                    "Identify recurring concept patterns",
                    "Assess confidence levels by difficulty band",
                    "Detect avoidance behaviors (skipping patterns)",
                    "Evaluate concept coverage gaps"
                ]
            }
        }
    
    def _get_existing_concept_aliases(self, user_id: str) -> List[Dict[str, Any]]:
        """Get existing concept alias mappings for the user"""
        db = SessionLocal()
        try:
            # Check if we have any existing alias mappings
            aliases = db.execute(text("""
                SELECT semantic_id, canonical_label, members
                FROM concept_alias_map_latest
                WHERE user_id = :user_id
                ORDER BY created_at DESC
            """), {"user_id": user_id}).fetchall()
            
            alias_list = []
            for alias in aliases:
                # Parse members JSON
                try:
                    members = alias.members if isinstance(alias.members, list) else json.loads(alias.members or "[]")
                except:
                    members = []
                
                alias_list.append({
                    "semantic_id": alias.semantic_id,
                    "canonical_label": alias.canonical_label,
                    "members": members
                })
            
            return alias_list
            
        except Exception as e:
            logger.warning(f"Could not load existing aliases for user {user_id[:8]}: {e}")
            return []
        finally:
            db.close()
    
    def _generate_empty_summary(self) -> Dict[str, Any]:
        """Generate empty summary for users with no history"""
        return {
            "concept_alias_map_updated": [],
            "dominance_by_item": {},
            "concept_readiness_labels": [],
            "pair_coverage_labels": [],
            "notes": "No attempt history available for analysis"
        }
    
    def _calculate_alias_reuse_rate(self, data: Dict[str, Any]) -> float:
        """Calculate the rate of alias reuse vs new concept creation"""
        concept_map = data.get("concept_alias_map_updated", [])
        if not concept_map:
            return 0.0
        
        reused_count = sum(1 for concept in concept_map if not concept.get("provisional", False))
        total_count = len(concept_map)
        
        return reused_count / total_count if total_count > 0 else 0.0
    
    def _count_provisional_concepts(self, data: Dict[str, Any]) -> int:
        """Count the number of provisional (new) concepts created"""
        concept_map = data.get("concept_alias_map_updated", [])
        return sum(1 for concept in concept_map if concept.get("provisional", False))
    
    def save_session_summary(self, user_id: str, session_id: str, summary_data: Dict[str, Any]) -> None:
        """Save summarizer results to database"""
        db = SessionLocal()
        try:
            # Save to session_summary_llm table with CORRECT column names
            db.execute(text("""
                INSERT INTO session_summary_llm (
                    user_id, session_id, 
                    concept_alias_map, dominance,
                    readiness_reasons, coverage_labels,
                    llm_model_used, created_at
                ) VALUES (
                    :user_id, :session_id,
                    :concept_alias_map_json, :dominance_json,
                    :readiness_reasons_json, :coverage_labels_json,
                    :llm_model_used, CURRENT_TIMESTAMP
                )
            """), {
                "user_id": user_id,
                "session_id": session_id,
                "concept_alias_map_json": json.dumps(summary_data.get("concept_alias_map_updated", [])),
                "dominance_json": json.dumps(summary_data.get("dominance_by_item", {})),
                "readiness_reasons_json": json.dumps(summary_data.get("concept_readiness_labels", [])),
                "coverage_labels_json": json.dumps(summary_data.get("pair_coverage_labels", [])),
                "llm_model_used": "openai:gpt-4o-mini"
            })
            
            # Update concept alias map latest - simplified upsert per user
            if summary_data.get("concept_alias_map_updated"):
                db.execute(text("""
                    INSERT INTO concept_alias_map_latest (
                        user_id, semantic_id, canonical_label, members, last_updated, usage_count
                    ) 
                    SELECT 
                        :user_id, 
                        alias->>'semantic_id',
                        alias->>'canonical_label', 
                        alias->'members',
                        CURRENT_TIMESTAMP,
                        1
                    FROM jsonb_array_elements(:alias_map_json::jsonb) AS alias
                    ON CONFLICT (user_id, semantic_id) 
                    DO UPDATE SET 
                        canonical_label = EXCLUDED.canonical_label,
                        members = EXCLUDED.members,
                        last_updated = EXCLUDED.last_updated,
                        usage_count = concept_alias_map_latest.usage_count + 1
                """), {
                    "user_id": user_id,
                    "alias_map_json": json.dumps(summary_data.get("concept_alias_map_updated", []))
                })
            
            db.commit()
            logger.info(f"💾 Saved summarizer results for user {user_id[:8]}")
            
        except Exception as e:
            logger.error(f"❌ Failed to save summarizer results: {e}")
            db.rollback()
        finally:
            db.close()

# Global instance
summarizer_service = SummarizerService()