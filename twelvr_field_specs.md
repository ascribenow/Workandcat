# Twelvr Field Specifications
**Adaptive Engine Data Documentation**

**Generated:** December 2024  
**Tables Analyzed:** `questions` (452 rows), `pyq_questions` (236 rows), `users` (references only)  
**Purpose:** Authoritative field specs for adaptive selection, bridges, and PYQ-pulse sessions

---

## A) Field Specifications

### Field: difficulty_score
**Table(s):** both  
**Type & Range:** NUMERIC(3,2) 1.20..3.50 (questions), 1.50..3.20 (pyq_questions)  
**Nullability & Defaults:** nullable, no default  
**Definition (plain words):** LLM-generated numerical assessment of question difficulty on a 1-5 scale based on conceptual complexity, computational intensity, and reasoning depth required  
**Computation/Formula (authoritative):**
- Generated by LLM (GPT-4o/GPT-4o-mini) during enrichment process
- Uses multi-criteria assessment: conceptual complexity, computational steps, reasoning chains
- Applied during `regular_enrichment_service.py` and `pyq_enrichment_service.py` processing
- No post-processing normalization applied
**Buckets/Thresholds (if any):** 
- Easy: ≤ 2.0  
- Medium: 2.1 - 3.5  
- Hard: ≥ 3.6  
**Controlled Vocabulary (if categorical):** N/A (continuous numeric)  
**Refresh Cadence & Source of Truth:** Set once during LLM enrichment, manual re-enrichment via admin UI  
**QA/Validation:** LLM self-assessment, no explicit validation rules  
**Edge Cases / Caveats:** 
- questions: 0 nulls, 7 distinct values, mostly 2.5-3.2 range
- pyq_questions: 1 null (0.4%), 5 distinct values
- Scores rarely exceed 3.5 in practice
**Last Updated & Owner:** During enrichment pipeline, managed by regular_enrichment_service.py  
**Examples (5 rows per table where non-null):**
- questions: id:abc123, value:3.0, subcategory:"Time-Speed-Distance", type_of_question:"Relative Speed"
- questions: id:def456, value:2.5, subcategory:"Percentages", type_of_question:"Basics"
- questions: id:ghi789, value:1.5, subcategory:"Averages and Alligation", type_of_question:"Basic Averages"
- pyq_questions: id:pqr123, value:3.0, subcategory:"Circles", type_of_question:"Tangents & Chords"
- pyq_questions: id:stu456, value:2.5, subcategory:"Time-Work", type_of_question:"Work Time Effeciency"

### Field: difficulty_band
**Table(s):** both  
**Type & Range:** VARCHAR(20) enum {Easy, Medium}  
**Nullability & Defaults:** nullable, no default  
**Definition (plain words):** Categorical representation of difficulty_score divided into discrete bands  
**Computation/Formula (authoritative):**
- Derived from difficulty_score during LLM enrichment
- Logic: Easy if difficulty_score ≤ 2.0, Medium if > 2.0
- Hard band exists in thresholds but no data shows Hard (≥ 3.6) values in current dataset
**Buckets/Thresholds (if any):**
- Easy: difficulty_score ≤ 2.0
- Medium: difficulty_score > 2.0 (and < 3.6)
- Hard: difficulty_score ≥ 3.6 (no current data)
**Controlled Vocabulary (if categorical):** {"Easy", "Medium", "Hard"}  
**Refresh Cadence & Source of Truth:** Updated when difficulty_score changes during enrichment  
**QA/Validation:** Automatically derived from difficulty_score, consistency enforced  
**Edge Cases / Caveats:**
- questions: Easy=52, Medium=400 (no Hard values)
- pyq_questions: Easy=4, Medium=231 (no Hard values)
- Despite Hard threshold existing, no questions exceed 3.6 difficulty_score
**Last Updated & Owner:** Derived during enrichment, managed by enrichment services  
**Examples (5 rows per table where non-null):**
- questions: id:abc123, value:"Medium", difficulty_score:3.0
- questions: id:def456, value:"Easy", difficulty_score:1.5
- pyq_questions: id:pqr123, value:"Medium", difficulty_score:3.0
- pyq_questions: id:stu456, value:"Easy", difficulty_score:1.5

### Field: pyq_frequency_score
**Table(s):** questions only  
**Type & Range:** NUMERIC(5,4) 0.5000..1.5000  
**Nullability & Defaults:** nullable, no default  
**Definition (plain words):** LLM-calculated frequency score indicating how often similar questions appear in PYQ bank, used for adaptive selection  
**Computation/Formula (authoritative):**
- Generated by `regular_enrichment_service._calculate_pyq_frequency_score_llm()`
- Logic: If difficulty_score ≤ 1.5 → return 0.5 (easy questions get low frequency)
- If difficulty_score > 1.5 → LLM compares against ALL category×subcategory matched PYQs
- Uses GPT-4o/GPT-4o-mini with fallback chain for LLM-based similarity assessment
- Currently being recalculated via corrected logic (manual script running)
**Buckets/Thresholds (if any):**
- 0.5: Low frequency (easy questions, difficulty ≤ 1.5)
- 1.0: Medium frequency 
- 1.5: High frequency
**Controlled Vocabulary (if categorical):** {0.5, 1.0, 1.5} (discrete values only)  
**Refresh Cadence & Source of Truth:** Manual trigger via admin UI "Recalculate Frequency" button, background job processing  
**QA/Validation:** LLM-based similarity assessment, no explicit validation  
**Edge Cases / Caveats:**
- Only exists in questions table, not pyq_questions
- 3 distinct values only: 0.5, 1.0, 1.5
- Mean: 0.968, heavily weighted toward 1.0
- Current recalculation fixing previous logic bug (difficulty filtering issue)
**Last Updated & Owner:** Background job via `/api/admin/recalculate-frequency-background`, managed by regular_enrichment_service.py  
**Examples (5 rows per table where non-null):**
- questions: id:abc123, value:1.0, difficulty_score:3.0, category:"Arithmetic"
- questions: id:def456, value:0.5, difficulty_score:1.5, category:"Arithmetic"
- questions: id:ghi789, value:1.5, difficulty_score:3.2, category:"Geometry and Mensuration"

### Field: concept_difficulty
**Table(s):** both  
**Type & Range:** TEXT (free-form LLM-generated descriptions)  
**Nullability & Defaults:** nullable, no default  
**Definition (plain words):** LLM-generated detailed explanation of what makes the question conceptually challenging  
**Computation/Formula (authoritative):**
- Generated during LLM enrichment process using GPT-4o/GPT-4o-mini
- Prompt asks LLM to analyze conceptual complexity factors
- No standardized format, free-form descriptive text
- Part of comprehensive enrichment pipeline
**Buckets/Thresholds (if any):** N/A (free-form text)  
**Controlled Vocabulary (if categorical):** N/A (unique per question)  
**Refresh Cadence & Source of Truth:** Set during enrichment, manual re-enrichment possible  
**QA/Validation:** Part of 22-criteria quality verification process  
**Edge Cases / Caveats:**
- questions: 444 distinct values (mostly unique)
- pyq_questions: 234 distinct values, 1 null
- Highly variable content and length
**Last Updated & Owner:** LLM enrichment services during processing  
**Examples (5 rows per table where non-null):**
- questions: id:abc123, value:"Requires understanding of relative motion and simultaneous equations"
- pyq_questions: id:pqr123, value:"Complex circular motion with tangent properties"

### Field: operations_required
**Table(s):** both  
**Type & Range:** TEXT (JSON array format)  
**Nullability & Defaults:** nullable, no default  
**Definition (plain words):** LLM-generated list of mathematical operations and techniques needed to solve the question  
**Computation/Formula (authoritative):**
- Generated by LLM during enrichment as structured list
- Typically stored as JSON array or comma-separated list
- Identifies specific mathematical operations, formulas, techniques required
**Buckets/Thresholds (if any):** N/A (categorical list)  
**Controlled Vocabulary (if categorical):** Variable, includes operations like "algebra", "geometry", "arithmetic", "percentage calculations", etc.  
**Refresh Cadence & Source of Truth:** Set during LLM enrichment  
**QA/Validation:** Part of quality verification process  
**Edge Cases / Caveats:**
- questions: 220 distinct combinations
- pyq_questions: 152 distinct combinations, 1 null
- Format varies (JSON vs comma-separated)
**Last Updated & Owner:** LLM enrichment pipeline  
**Examples (5 rows per table where non-null):**
- questions: id:abc123, value:"['algebra', 'percentage calculations', 'equation solving']"
- pyq_questions: id:pqr123, value:"['geometry', 'trigonometry', 'area calculations']"

### Field: problem_structure
**Table(s):** both  
**Type & Range:** VARCHAR(200) (questions), VARCHAR(200) (pyq_questions)  
**Nullability & Defaults:** nullable, no default  
**Definition (plain words):** LLM-identified structural pattern or template of the problem type  
**Computation/Formula (authoritative):**
- Generated by LLM during enrichment
- Identifies common problem patterns, question structures
- Limited to 200 characters
**Buckets/Thresholds (if any):** N/A (categorical descriptions)  
**Controlled Vocabulary (if categorical):** Variable structural patterns  
**Refresh Cadence & Source of Truth:** Set during LLM enrichment  
**QA/Validation:** Part of quality verification  
**Edge Cases / Caveats:**
- questions: 280 distinct values
- pyq_questions: 205 distinct values, 1 null
- Character limit may truncate complex descriptions
**Last Updated & Owner:** LLM enrichment services  
**Examples (5 rows per table where non-null):**
- questions: id:abc123, value:"Multi-step calculation with ratio comparison"
- pyq_questions: id:pqr123, value:"Geometric proof with angle relationships"

### Field: solution_method
**Table(s):** both  
**Type & Range:** VARCHAR(500) (questions), VARCHAR(200) (pyq_questions)  
**Nullability & Defaults:** nullable, no default  
**Definition (plain words):** LLM-suggested approach or method for solving the question  
**Computation/Formula (authoritative):**
- Generated by LLM during enrichment process
- Provides high-level solution strategy
- Different length limits: 500 chars (questions) vs 200 chars (pyq_questions)
**Buckets/Thresholds (if any):** N/A (descriptive text)  
**Controlled Vocabulary (if categorical):** Variable solution approaches  
**Refresh Cadence & Source of Truth:** Set during LLM enrichment  
**QA/Validation:** Part of comprehensive quality verification  
**Edge Cases / Caveats:**
- questions: 283 distinct values
- pyq_questions: 228 distinct values, 1 null
- Different character limits between tables may cause inconsistency
**Last Updated & Owner:** LLM enrichment pipeline  
**Examples (5 rows per table where non-null):**
- questions: id:abc123, value:"Set up equations for relative speeds and solve simultaneously"
- pyq_questions: id:pqr123, value:"Apply circle theorems and calculate angles"

### Field: core_concepts
**Table(s):** both  
**Type & Range:** TEXT (JSON array format)  
**Nullability & Defaults:** nullable, no default  
**Definition (plain words):** LLM-extracted list of fundamental mathematical concepts tested by the question  
**Computation/Formula (authoritative):**
- Generated by LLM during enrichment using concept extraction prompts
- Stored as JSON array or structured list
- Used for concept_extraction_status determination (non-empty = completed)
- Critical for adaptive learning and concept tracking
**Buckets/Thresholds (if any):** N/A (concept list)  
**Controlled Vocabulary (if categorical):** Mathematical concept taxonomy (variable)  
**Refresh Cadence & Source of Truth:** Set during LLM enrichment, drives concept_extraction_status  
**QA/Validation:** Non-empty core_concepts required for concept_extraction_status = "completed"  
**Edge Cases / Caveats:**
- questions: 345 distinct combinations
- pyq_questions: 229 distinct combinations, 1 null
- Critical field for determining enrichment completion status
**Last Updated & Owner:** LLM enrichment services, concept extraction phase  
**Examples (5 rows per table where non-null):**
- questions: id:abc123, value:"['relative motion', 'linear equations', 'time-distance relationships']"
- pyq_questions: id:pqr123, value:"['circle geometry', 'tangent properties', 'angle calculations']"

### Field: importance_band
**Table(s):** DELETED  
**Type & Range:** N/A (column dropped during cleanup)  
**Nullability & Defaults:** N/A  
**Definition (plain words):** Previously categorized question importance, removed during database cleanup  
**Computation/Formula (authoritative):** DELETED - column dropped from database  
**Buckets/Thresholds (if any):** N/A  
**Controlled Vocabulary (if categorical):** N/A  
**Refresh Cadence & Source of Truth:** N/A (deleted)  
**QA/Validation:** N/A  
**Edge Cases / Caveats:** Column was dropped as unused (all values were NULL)  
**Last Updated & Owner:** Deleted during database cleanup December 2024  
**Examples (5 rows per table where non-null):** N/A (field deleted)

### Field: answer_match
**Table(s):** questions only  
**Type & Range:** BOOLEAN  
**Nullability & Defaults:** nullable, default=false  
**Definition (plain words):** LLM-based semantic validation indicating whether the provided answer correctly matches the question using semantic comparison  
**Computation/Formula (authoritative):**
- Generated during LLM enrichment process
- Uses semantic answer comparison between question.answer and expected response
- Part of quality verification (22-criteria checklist)
- Boolean result from LLM validation
**Buckets/Thresholds (if any):** true/false  
**Controlled Vocabulary (if categorical):** {true, false}  
**Refresh Cadence & Source of Truth:** Set during LLM enrichment  
**QA/Validation:** Part of comprehensive quality verification process  
**Edge Cases / Caveats:**
- Only exists in questions table (not in pyq_questions)
- True: 388 rows, False: 64 rows
- High success rate (85.8% true)
**Last Updated & Owner:** LLM enrichment pipeline, managed by regular_enrichment_service.py  
**Examples (5 rows per table where non-null):**
- questions: id:abc123, value:true, answer:"15 km/hr"
- questions: id:def456, value:false, answer:"Invalid format"

### Field: concept_extraction_status
**Table(s):** both  
**Type & Range:** VARCHAR(50) (questions), VARCHAR(100) (pyq_questions) enum {pending, completed}  
**Nullability & Defaults:** nullable, default='pending'  
**Definition (plain words):** Status indicator for LLM concept extraction completion, based on core_concepts field population  
**Computation/Formula (authoritative):**
- Set to "completed" when core_concepts field is non-empty
- Set to "pending" when core_concepts is empty or null
- Logic implemented in enrichment services
- Critical for quality verification eligibility
**Buckets/Thresholds (if any):** 
- "pending": core_concepts empty/null
- "completed": core_concepts populated
**Controlled Vocabulary (if categorical):** {"pending", "completed"}  
**Refresh Cadence & Source of Truth:** Updated during enrichment process  
**QA/Validation:** Directly tied to core_concepts field presence  
**Edge Cases / Caveats:**
- questions: completed=370, pending=82
- pyq_questions: completed=235, pending=1
- Must be "completed" for quality_verified=true eligibility
**Last Updated & Owner:** Enrichment services during concept extraction phase  
**Examples (5 rows per table where non-null):**
- questions: id:abc123, value:"completed", core_concepts:"['motion', 'algebra']"
- questions: id:def456, value:"pending", core_concepts:null
- pyq_questions: id:pqr123, value:"completed", core_concepts:"['geometry']"

### Field: quality_verified
**Table(s):** both  
**Type & Range:** BOOLEAN  
**Nullability & Defaults:** nullable, default=false  
**Definition (plain words):** Boolean flag indicating question has passed comprehensive 22-criteria quality verification process  
**Computation/Formula (authoritative):**
- Set to true when question passes all 22 quality criteria
- Criteria include: concept_extraction_status="completed", answer_match validation, field completeness
- Prerequisite for adaptive selection eligibility
- Updated during enrichment pipeline final stage
**Buckets/Thresholds (if any):** true/false  
**Controlled Vocabulary (if categorical):** {true, false}  
**Refresh Cadence & Source of Truth:** Updated during enrichment quality verification phase  
**QA/Validation:** Based on 22-criteria checklist (updated from 21 to include concept_extraction_status)  
**Edge Cases / Caveats:**
- questions: True=370, False=82 (81.9% pass rate)
- pyq_questions: True=229, False=7 (97.0% pass rate)
- Required for questions to be used in adaptive sessions
**Last Updated & Owner:** Quality verification phase of enrichment services  
**Examples (5 rows per table where non-null):**
- questions: id:abc123, value:true, concept_extraction_status:"completed"
- questions: id:def456, value:false, concept_extraction_status:"pending"
- pyq_questions: id:pqr123, value:true, concept_extraction_status:"completed"

### Field: subcategory
**Table(s):** both  
**Type & Range:** TEXT  
**Nullability & Defaults:** NOT NULL  
**Definition (plain words):** Fine-grained topic classification within broader category, used for PYQ matching and content organization  
**Computation/Formula (authoritative):**
- Set during CSV upload or LLM enrichment
- Used for category×subcategory matching in pyq_frequency_score calculation
- Part of canonical taxonomy structure
**Buckets/Thresholds (if any):** N/A (categorical taxonomy)  
**Controlled Vocabulary (if categorical):** 
- questions: 16 distinct (Time-Speed-Distance=140, Time-Work=76, Percentages=40, etc.)
- pyq_questions: 27 distinct (Time-Speed-Distance=35, Averages and Alligation=27, Time-Work=22, etc.)
**Refresh Cadence & Source of Truth:** Static after initial classification  
**QA/Validation:** Part of canonical taxonomy validation  
**Edge Cases / Caveats:**
- Critical for PYQ frequency matching logic
- Different distribution between questions and pyq_questions tables
**Last Updated & Owner:** Set during data ingestion/enrichment  
**Examples (5 rows per table where non-null):**
- questions: "Time-Speed-Distance", "Time-Work", "Percentages", "Profit-Loss-Discount", "Averages and Alligation"
- pyq_questions: "Time-Speed-Distance", "Averages and Alligation", "Time-Work", "Ratios and Proportions", "Profit-Loss-Discount"

### Field: type_of_question
**Table(s):** both  
**Type & Range:** VARCHAR(150) (questions), VARCHAR(250) (pyq_questions)  
**Nullability & Defaults:** nullable, no default  
**Definition (plain words):** Specific question type classification within subcategory, provides granular categorization for adaptive selection  
**Computation/Formula (authoritative):**
- Set during enrichment or data ingestion
- More specific than subcategory, describes question variant
- Different character limits between tables
**Buckets/Thresholds (if any):** N/A (categorical)  
**Controlled Vocabulary (if categorical):**
- questions: 33 distinct (Basics=151, Work Time Effeciency=58, Relative Speed=46, etc.)
- pyq_questions: 44 distinct (Basics=51, Work Time Effeciency=19, Basic Averages=16, etc.)
**Refresh Cadence & Source of Truth:** Set during classification phase  
**QA/Validation:** Part of taxonomy validation  
**Edge Cases / Caveats:**
- Different character limits may affect data consistency
- 1 null in pyq_questions table
**Last Updated & Owner:** Classification during enrichment  
**Examples (5 rows per table where non-null):**
- questions: "Basics", "Work Time Effeciency", "Relative Speed", "Tangents & Chords", "Percentage Change"
- pyq_questions: "Basics", "Work Time Effeciency", "Basic Averages", "Relative Speed", "Compound Ratios"

### Field: category
**Table(s):** both  
**Type & Range:** VARCHAR(100) (questions), VARCHAR(255) (pyq_questions)  
**Nullability & Defaults:** nullable, no default  
**Definition (plain words):** Top-level subject classification for mathematical domain organization  
**Computation/Formula (authoritative):**
- Set during data classification
- Highest level of content taxonomy
- Used in PYQ frequency matching (category×subcategory pairs)
**Buckets/Thresholds (if any):** N/A (categorical taxonomy)  
**Controlled Vocabulary (if categorical):**
- questions: 3 distinct (Arithmetic=382, Geometry and Mensuration=63, Algebra=7)
- pyq_questions: 6 distinct (Arithmetic=160, Algebra=28, Geometry and Mensuration=27, Number System=8, To be classified by LLM=6, Modern Math=6)
**Refresh Cadence & Source of Truth:** Static after classification  
**QA/Validation:** Part of canonical taxonomy  
**Edge Cases / Caveats:**
- Different distributions between tables
- pyq_questions has "To be classified by LLM" indicating incomplete classification
- 1 null in pyq_questions
**Last Updated & Owner:** Set during taxonomy classification  
**Examples (5 rows per table where non-null):**
- questions: "Arithmetic", "Geometry and Mensuration", "Algebra"
- pyq_questions: "Arithmetic", "Algebra", "Geometry and Mensuration", "Number System", "Modern Math"

---

## B) Distribution Snapshots

### Numeric Fields:

**difficulty_score:**
- questions: Count=452, NULL=0, Min=1.20, Max=3.50, Mean=2.862, Std=0.546, P10/P50/P90=1.50/3.00/3.20
- pyq_questions: Count=235, NULL=1, Min=1.50, Max=3.20, Mean=2.813, Std=0.288, P10/P50/P90=2.50/3.00/3.00

**pyq_frequency_score:**
- questions: Count=452, NULL=0, Min=0.5000, Max=1.5000, Mean=0.968, Std=0.181, P10/P50/P90=1.00/1.00/1.00
- pyq_questions: Field does not exist

### Band/Bucket Distributions:

**difficulty_band:**
- questions: Easy=52 (11.5%), Medium=400 (88.5%), Hard=0 (0%)
- pyq_questions: Easy=4 (1.7%), Medium=231 (97.9%), Hard=0 (0%)

**pyq_frequency_score:**
- questions: 0.5=UNKNOWN count, 1.0=UNKNOWN count, 1.5=UNKNOWN count (currently being recalculated)

---

## C) Controlled Vocabulary Dumps

### questions table:

**difficulty_band:** "Medium" (400), "Easy" (52)
**concept_extraction_status:** "completed" (370), "pending" (82)
**quality_verified:** "True" (370), "False" (82)
**answer_match:** "True" (388), "False" (64)
**category:** "Arithmetic" (382), "Geometry and Mensuration" (63), "Algebra" (7)
**subcategory:** "Time-Speed-Distance" (140), "Time-Work" (76), "Percentages" (40), "Profit-Loss-Discount" (36), "Averages and Alligation" (31), "Circles" (24), "Mixtures and Solutions" (23), "Ratios and Proportions" (21), "Mensuration 3D" (17), "Triangles" (16), [6 more values...]
**type_of_question:** "Basics" (151), "Work Time Effeciency" (58), "Relative Speed" (46), "Tangents & Chords" (19), "Percentage Change" (17), "Pipes and Cisterns" (17), "Concentration Change" (16), "Properties (Angles, Sides, Medians, Bisectors)" (15), "Compound Ratios" (14), "Weighted Averages" (11), [23 more values...]

### pyq_questions table:

**difficulty_band:** "Medium" (231), "Easy" (4)
**concept_extraction_status:** "completed" (235), "pending" (1)
**quality_verified:** "True" (229), "False" (7)
**category:** "Arithmetic" (160), "Algebra" (28), "Geometry and Mensuration" (27), "Number System" (8), "To be classified by LLM" (6), "Modern Math" (6)
**subcategory:** "Time-Speed-Distance" (35), "Averages and Alligation" (27), "Time-Work" (22), "Ratios and Proportions" (20), "Profit-Loss-Discount" (16), "Mixtures and Solutions" (14), "Simple and Compound Interest" (13), "Circles" (13), "Percentages" (12), "Linear Equations" (10), [17 more values...]
**type_of_question:** "Basics" (51), "Work Time Effeciency" (19), "Basic Averages" (16), "Relative Speed" (12), "Compound Ratios" (11), "Two variable systems" (9), "Tangents & Chords" (9), "Roots & Nature of Roots" (9), "Concentration Change" (8), "Alligations & Mixtures" (7), [34 more values...]

---

## D) Consistency Rules

### difficulty_score → difficulty_band Mapping:
- Easy: difficulty_score ≤ 2.0
- Medium: 2.0 < difficulty_score < 3.6
- Hard: difficulty_score ≥ 3.6
- **Current Data:** No Hard values exist despite threshold definition

### pyq_frequency_score Rules:
- **Values:** Exactly {0.5, 1.0, 1.5} (discrete only)
- **Meanings:** 0.5=Low frequency, 1.0=Medium frequency, 1.5=High frequency
- **Logic:** If difficulty_score ≤ 1.5 → 0.5, else LLM comparison with category×subcategory matched PYQs
- **Lookback Window:** All PYQs in same category×subcategory (no time window limit)
- **Current Status:** Being recalculated due to previous logic bug (fixed difficulty filtering)

### Per-topic Normalization:
- No per-subcategory or type_of_question normalization rules found
- Difficulty thresholds applied uniformly across all topics
- PYQ frequency matching uses exact category×subcategory pairs

### Quality Verification Dependencies:
- quality_verified=true requires concept_extraction_status="completed"
- concept_extraction_status="completed" requires non-empty core_concepts
- 22-criteria checklist includes concept_extraction_status (updated from 21)

---

## E) Sample Data Extracts
**Files Generated:**
- `/app/questions_field_samples.csv` (50 rows, 16 fields)
- `/app/pyq_questions_field_samples.csv` (50 rows, 14 fields)

Selection rule: Quality-verified rows with varied difficulty_score, subcategory, type_of_question values to illustrate field diversity.

---

## F) Gaps List

### Definition Gaps:
1. **pyq_frequency_score exact bucketing logic:** Currently being recalculated, final distribution unknown
2. **Core concept taxonomy:** No standardized vocabulary found for core_concepts values
3. **Operations required format:** Inconsistent JSON vs comma-separated storage format
4. **Solution method character limits:** Different limits between tables (500 vs 200 chars) may cause truncation

### Formula Gaps:
1. **22-criteria quality verification:** Complete checklist not documented in code
2. **LLM model versions:** Specific GPT-4o vs GPT-4o-mini selection logic not documented
3. **Semantic answer matching:** Exact LLM prompt and threshold for answer_match unknown

### Threshold Gaps:
1. **pyq_frequency_score thresholds:** 0.5/1.0/1.5 meanings and LLM similarity cutoffs undocumented
2. **Concept extraction completion criteria:** Exact requirements for core_concepts to be considered "complete"

### Owner Suggestions:
- **pyq_frequency_score:** regular_enrichment_service.py maintainer
- **Quality verification:** Enhanced enrichment checker service team
- **Taxonomy standardization:** Content team + LLM engineering team
- **Field format consistency:** Database schema team

---

**Last Updated:** December 2024  
**Next Review:** After pyq_frequency_score recalculation completion  
**Contact:** Engineering team for technical details, Content team for taxonomy questions