#!/usr/bin/env python3
"""
COMPREHENSIVE ADAPTIVE ENGINE ANALYSIS
How Your Adaptive Learning System Works - Complete Technical Deep Dive
"""

import os
import sys
sys.path.append('/app/backend')

from sqlalchemy import create_engine, text
from dotenv import load_dotenv
from database import SessionLocal
import json

load_dotenv('/app/backend/.env')

def analyze_adaptive_engine():
    """Complete analysis of how the adaptive engine works"""
    
    print("üß† COMPREHENSIVE ADAPTIVE ENGINE ANALYSIS")
    print("=" * 80)
    print("HOW YOUR TWELVR ADAPTIVE LEARNING SYSTEM WORKS")
    print("=" * 80)
    
    db = SessionLocal()
    
    try:
        # SECTION 1: ADAPTIVE ARCHITECTURE OVERVIEW
        print("\nüìã SECTION 1: ADAPTIVE ARCHITECTURE OVERVIEW")
        print("-" * 60)
        
        print("""
üéØ YOUR ADAPTIVE SYSTEM ARCHITECTURE:

1. **V2 ADAPTIVE PIPELINE** (Fast & Deterministic)
   ‚îú‚îÄ‚îÄ Cold-Start Detection (0 sessions) 
   ‚îú‚îÄ‚îÄ User History Analysis (session count)
   ‚îú‚îÄ‚îÄ Deterministic Candidate Selection (pool building)
   ‚îú‚îÄ‚îÄ LLM Planning (with fallback)
   ‚îî‚îÄ‚îÄ Pack Assembly (12-question optimization)

2. **PACK-BASED SERVING** (Frontend Optimization)
   ‚îú‚îÄ‚îÄ Pre-assembled 12-question packs
   ‚îú‚îÄ‚îÄ JSON array MCQ format
   ‚îú‚îÄ‚îÄ No real-time database queries
   ‚îî‚îÄ‚îÄ Session progress tracking

3. **ADAPTIVE-ONLY ARCHITECTURE** (Simplified)
   ‚îú‚îÄ‚îÄ No legacy session paths  
   ‚îú‚îÄ‚îÄ All users adaptive-enabled
   ‚îú‚îÄ‚îÄ Single code path maintenance
   ‚îî‚îÄ‚îÄ Pure adaptive learning experience
        """)
        
        # SECTION 2: QUESTION POOL ANALYSIS
        print("\n‚ùì SECTION 2: QUESTION POOL ANALYSIS") 
        print("-" * 60)
        
        result = db.execute(text("""
            SELECT 
                difficulty_band,
                COUNT(*) as questions,
                COUNT(CASE WHEN is_active = true THEN 1 END) as active,
                AVG(difficulty_score) as avg_difficulty,
                AVG(pyq_frequency_score) as avg_pyq_freq
            FROM questions 
            WHERE mcq_options IS NOT NULL
            GROUP BY difficulty_band
            ORDER BY 
                CASE difficulty_band 
                    WHEN 'Easy' THEN 1 
                    WHEN 'Medium' THEN 2 
                    WHEN 'Hard' THEN 3 
                    ELSE 4 
                END
        """))
        
        question_stats = result.fetchall()
        print("üéØ YOUR QUESTION POOL COMPOSITION:")
        total_questions = sum(row.questions for row in question_stats)
        
        for row in question_stats:
            percentage = (row.questions / total_questions) * 100
            print(f"""
   üìä {row.difficulty_band} Questions:
     - Total: {row.questions} ({percentage:.1f}% of pool)
     - Active: {row.active} 
     - Avg Difficulty Score: {row.avg_difficulty:.2f}/5.0
     - Avg PYQ Frequency: {row.avg_pyq_freq:.2f}/1.5
            """)
        
        # SECTION 3: ADAPTIVE SESSION FLOW
        print("\nüîÑ SECTION 3: ADAPTIVE SESSION FLOW")
        print("-" * 60)
        
        print("""
üöÄ HOW A USER SESSION WORKS:

**STEP 1: SESSION INITIATION**
User clicks "Today's Session" ‚Üí Frontend calls /api/adapt/plan-next

**STEP 2: COLD-START vs ADAPTIVE DETECTION**
‚îî‚îÄ‚îÄ If user has 0 completed sessions: COLD-START path
‚îî‚îÄ‚îÄ If user has 1+ sessions: ADAPTIVE path (uses attempt history)

**STEP 3: CANDIDATE POOL BUILDING** 
‚îú‚îÄ‚îÄ Cold-Start: Creates 100-question diversity pool
‚îú‚îÄ‚îÄ Adaptive: Analyzes user performance patterns
‚îú‚îÄ‚îÄ Applies PYQ constraints (min 2 questions ‚â•1.0 PYQ score)
‚îî‚îÄ‚îÄ Ensures difficulty distribution availability

**STEP 4: LLM PLANNING** (with Fallback)
‚îú‚îÄ‚îÄ Primary: OpenAI GPT-4o plans optimal 12 questions
‚îú‚îÄ‚îÄ Fallback: Google Gemini if GPT-4o fails
‚îú‚îÄ‚îÄ Final Fallback: Deterministic selection
‚îî‚îÄ‚îÄ Target: 3 Easy, 6 Medium, 3 Hard distribution

**STEP 5: PACK ASSEMBLY**
‚îú‚îÄ‚îÄ Fetches full question data (stem, MCQ options, solutions)
‚îú‚îÄ‚îÄ Converts to JSON array format for options
‚îú‚îÄ‚îÄ Creates V2PackItem structures
‚îî‚îÄ‚îÄ Stores complete pack in session_pack_plan table

**STEP 6: FRONTEND SERVING**
‚îú‚îÄ‚îÄ Frontend fetches complete pack via /api/adapt/pack
‚îú‚îÄ‚îÄ Displays questions sequentially from pack
‚îú‚îÄ‚îÄ No database queries during session
‚îî‚îÄ‚îÄ Answer comparison uses pack data only

**STEP 7: SESSION COMPLETION**
‚îú‚îÄ‚îÄ All answers logged to attempt_events
‚îú‚îÄ‚îÄ Session marked as completed
‚îú‚îÄ‚îÄ Triggers next session pre-planning
‚îî‚îÄ‚îÄ Analytics pipeline processes learning data
        """)
        
        # SECTION 4: USER JOURNEY ANALYSIS
        print("\nüë§ SECTION 4: USER JOURNEY ANALYSIS")
        print("-" * 60)
        
        result = db.execute(text("""
            SELECT 
                COUNT(DISTINCT user_id) as total_users,
                COUNT(DISTINCT CASE WHEN sess_seq = 1 THEN user_id END) as users_with_sessions,
                AVG(sess_seq) as avg_sessions_per_user,
                MAX(sess_seq) as max_sessions_by_user
            FROM sessions
            WHERE status = 'completed'
        """))
        
        user_stats = result.fetchone()
        
        print(f"""
üë• USER ENGAGEMENT METRICS:
   - Total Users in System: {user_stats.total_users}
   - Users Who Started Sessions: {user_stats.users_with_sessions} 
   - Average Sessions Per User: {user_stats.avg_sessions_per_user:.1f}
   - Most Sessions by Single User: {user_stats.max_sessions_by_user}
        """)
        
        # SECTION 5: PERFORMANCE CHARACTERISTICS
        print("\n‚ö° SECTION 5: PERFORMANCE CHARACTERISTICS")
        print("-" * 60)
        
        result = db.execute(text("""
            SELECT 
                COUNT(*) as total_attempts,
                COUNT(CASE WHEN was_correct = true THEN 1 END) as correct_attempts,
                COUNT(CASE WHEN skipped = true THEN 1 END) as skipped_attempts,
                AVG(response_time_ms) as avg_response_time
            FROM attempt_events
        """))
        
        performance = result.fetchone()
        accuracy_rate = (performance.correct_attempts / performance.total_attempts) * 100 if performance.total_attempts > 0 else 0
        
        print(f"""
üìä SYSTEM PERFORMANCE METRICS:
   - Total Question Attempts: {performance.total_attempts}
   - Correct Answers: {performance.correct_attempts} ({accuracy_rate:.1f}%)
   - Questions Skipped: {performance.skipped_attempts}
   - Avg Response Time: {performance.avg_response_time:.0f}ms per question
        """)
        
        # SECTION 6: ADAPTIVE INTELLIGENCE
        print("\nüß† SECTION 6: ADAPTIVE INTELLIGENCE") 
        print("-" * 60)
        
        print("""
üéØ HOW YOUR SYSTEM ADAPTS TO USERS:

**COLD-START INTELLIGENCE** (New Users):
‚îú‚îÄ‚îÄ Builds diverse 100-question candidate pool
‚îú‚îÄ‚îÄ Ensures coverage across all subcategories  
‚îú‚îÄ‚îÄ Applies difficulty distribution (25% Easy, 50% Medium, 25% Hard)
‚îú‚îÄ‚îÄ Includes high-PYQ frequency questions for CAT relevance
‚îî‚îÄ‚îÄ Creates balanced first session for baseline assessment

**ADAPTIVE INTELLIGENCE** (Returning Users):
‚îú‚îÄ‚îÄ Analyzes attempt_events for user performance patterns
‚îú‚îÄ‚îÄ Identifies weak areas (low accuracy subcategories)
‚îú‚îÄ‚îÄ Considers recent performance trends
‚îú‚îÄ‚îÄ Adjusts difficulty based on user success rate
‚îú‚îÄ‚îÄ Maintains CAT-relevant question selection
‚îî‚îÄ‚îÄ Optimizes for learning progression

**DETERMINISTIC KERNELS** (Consistency):
‚îú‚îÄ‚îÄ stable_semantic_id(): Ensures consistent question selection
‚îú‚îÄ‚îÄ weights_from_dominance(): Converts performance to selection weights  
‚îú‚îÄ‚îÄ finalize_readiness(): Applies learning readiness rules
‚îú‚îÄ‚îÄ validate_pack(): Ensures all constraints met
‚îî‚îÄ‚îÄ coverage_debt_by_sessions(): Tracks coverage across sessions
        """)
        
        # SECTION 7: DATA FLOW ANALYSIS
        print("\nüìä SECTION 7: DATA FLOW ANALYSIS")
        print("-" * 60)
        
        result = db.execute(text("""
            SELECT 
                'session_pack_plan' as table_name,
                COUNT(*) as records,
                pg_size_pretty(pg_total_relation_size('session_pack_plan')) as size
            UNION ALL
            SELECT 
                'attempt_events' as table_name,
                COUNT(*) as records, 
                pg_size_pretty(pg_total_relation_size('attempt_events')) as size
            FROM attempt_events
            UNION ALL
            SELECT
                'questions' as table_name,
                COUNT(*) as records,
                pg_size_pretty(pg_total_relation_size('questions')) as size  
            FROM questions
        """))
        
        data_flow = result.fetchall()
        
        print("üíæ CORE DATA TABLES:")
        for row in data_flow:
            print(f"   - {row.table_name}: {row.records} records ({row.size})")
        
        # SECTION 8: QUALITY ASSURANCE
        print("\n‚úÖ SECTION 8: QUALITY ASSURANCE")
        print("-" * 60)
        
        result = db.execute(text("""
            SELECT 
                COUNT(*) as total_questions,
                COUNT(CASE WHEN quality_verified = true THEN 1 END) as quality_verified,
                COUNT(CASE WHEN mcq_options IS NOT NULL THEN 1 END) as has_mcq_options,
                COUNT(CASE WHEN answer IS NOT NULL AND answer != '' THEN 1 END) as has_answers
            FROM questions
        """))
        
        quality = result.fetchone()
        
        print(f"""
üîç QUESTION QUALITY METRICS:
   - Total Questions: {quality.total_questions}
   - Quality Verified: {quality.quality_verified} ({(quality.quality_verified/quality.total_questions)*100:.1f}%)
   - Have MCQ Options: {quality.has_mcq_options} ({(quality.has_mcq_options/quality.total_questions)*100:.1f}%)
   - Have Clean Answers: {quality.has_answers} ({(quality.has_answers/quality.total_questions)*100:.1f}%)
        """)
        
        print("\n" + "=" * 80)
        print("üéâ ADAPTIVE ENGINE ANALYSIS COMPLETE!")
        print("=" * 80)
        
        # SECTION 9: STRATEGIC INSIGHTS 
        print("""
üöÄ KEY STRATEGIC INSIGHTS FOR YOUR ADAPTIVE ENGINE:

1. **FULLY ADAPTIVE SYSTEM**: 100% of users now get adaptive learning
2. **PERFORMANCE OPTIMIZED**: Single-query pack assembly, no legacy overhead
3. **CAT-FOCUSED**: PYQ frequency ensures exam relevance  
4. **DATA-DRIVEN**: Real attempt data drives future question selection
5. **SCALABLE ARCHITECTURE**: Pack-based serving handles concurrent users
6. **QUALITY ASSURED**: JSON array format ensures consistent MCQ handling

üéØ NEXT LEVEL OPTIMIZATION OPPORTUNITIES:

A. **DIFFICULTY ADAPTATION**: Fine-tune difficulty progression based on user accuracy
B. **CONCEPT MASTERY**: Track mastery by subcategory for targeted practice  
C. **TIMING OPTIMIZATION**: Use response_time_ms for adaptive time pressure
D. **WEAKNESS DETECTION**: Enhanced analytics for learning gap identification
E. **SPACED REPETITION**: Implement optimal review scheduling
F. **PERFORMANCE PREDICTION**: ML models for CAT score prediction

Your adaptive engine is production-ready and highly sophisticated! üéì
        """)
        
    except Exception as e:
        print(f"‚ùå Error during analysis: {e}")
        
    finally:
        db.close()

if __name__ == "__main__":
    analyze_adaptive_engine()